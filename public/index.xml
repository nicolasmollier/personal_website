<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nicolas Mollier</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Nicolas Mollier</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>de</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hubb2c620e44f44b05f57a49afd6fd01f1_2551_512x512_fill_lanczos_center_2.png</url>
      <title>Nicolas Mollier</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Twitter Analyse: Textanalyse der Tweets der Kandidaten um das Amt des Präsidenten der USA</title>
      <link>/post/twitter-analyse-textanalyse-der-tweets-der-kandidaten-um-das-amt-des-prasidenten-der-usa/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/post/twitter-analyse-textanalyse-der-tweets-der-kandidaten-um-das-amt-des-prasidenten-der-usa/</guid>
      <description>


&lt;p&gt;In dieser Analyse geht es darum die aktuellsten Tweets der Präsidentschaftskandidaten der USA von Twitter zu “scrapen”, die am häufigsten verwendeten Worte zu visualieren und eine inhaltliche Analyse der Tweets mittels Topic Modeling und Sentiment Analysis durchzuführen.&lt;/p&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Packages&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)
library(rtweet)
library(httpuv)
library(janitor)
library(qdapRegex)
library(tm)
library(qdap)
library(topicmodels)
library(syuzhet)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;scrape-the-data-from-twitter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scrape the Data from Twitter&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_tweets &amp;lt;- 3000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Als erstes laden wir die letzten 3000 Tweets der beiden Präsidentschaftskandidaten. Dazu nutzen wir die Pakete &lt;em&gt;rtweet&lt;/em&gt; und &lt;em&gt;httpuv&lt;/em&gt;, um über das Twitter API Zugriff auf die Tweets und die Profilinformationen zu erhalten.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;influence_canidates_df &amp;lt;- lookup_users(c(&amp;quot;realDonaldTrump&amp;quot;, &amp;quot;JoeBiden&amp;quot;))

trump_timeline_list &amp;lt;- list()
max_id &amp;lt;- NULL

for(i in 1:10){
  trump_timeline_list[[i]] &amp;lt;- get_timeline(user = &amp;quot;realDonaldTrump&amp;quot;, 
                                           n = n_tweets, 
                                           max_id = max_id)
  max_id &amp;lt;- min(trump_timeline_list[[i]]$status_id)
}

trump_timeline &amp;lt;- bind_rows(trump_timeline_list) %&amp;gt;% 
  distinct(status_id, .keep_all = TRUE)

n_tweets &amp;lt;- nrow(trump_timeline)
biden_timeline &amp;lt;- get_timeline(user = &amp;quot;JoeBiden&amp;quot;, n = n_tweets)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Die Funktion &lt;em&gt;get_timeline()&lt;/em&gt; funktioniert bei Joe Biden sehr gut. Bei Donald Trump liefert sie jedoch stets weniger als die angefragte Anzahl an Tweets. Zudem schwankt die Anzahl der Tweets sehr. Das ist der Grund. Das ist der Grund, warum im Fall von Donald Trump die Konstruktion mit einer for-Schleife gewählt wurde. Sie versucht, nach und nach die noch nicht erhaltenen Tweets zu laden und anschließend alle erhaltenen Tweets in einem Datensatz &lt;em&gt;trump_timeline&lt;/em&gt; zu vereinigen. So wird die Anzahl der erhaltenen Tweets zwar von nur einigen Hundert auf etwa 1500 erhöht. Die geforderte Anzahl von 2386 wird aber dennoch nicht erreicht. Für dieses Problem habe ich noch keine Lösung gefunden.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;influence-of-candidates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Influence of Candidates&lt;/h2&gt;
&lt;p&gt;Bevor wir mit der Analyse der Tweets beginnen, werfen wir einen kurzen Blick darauf, welcher der Kandidaten die meisten Follower hat.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_followers &amp;lt;- influence_canidates_df %&amp;gt;% 
  select(screen_name, followers_count) %&amp;gt;% 
  arrange(desc(followers_count)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_followers_trump &amp;lt;- n_followers %&amp;gt;% 
  filter(screen_name == &amp;quot;realDonaldTrump&amp;quot;) %&amp;gt;% 
  select(followers_count) %&amp;gt;% 
  as_vector()

n_followers_biden &amp;lt;- n_followers %&amp;gt;% 
  filter(screen_name == &amp;quot;JoeBiden&amp;quot;) %&amp;gt;% 
  select(followers_count) %&amp;gt;% 
  as_vector()

more_foll &amp;lt;- (n_followers_trump / n_followers_biden) %&amp;gt;% 
  round(digits = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Donald Trump hat mit 85880045 rund 9-mal so viele Follower wie Joe Biden.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;twitter-frequenz&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Twitter Frequenz&lt;/h2&gt;
&lt;p&gt;Als nächstes betrachten wir, welcher der beiden Kandidaten pro Tag mehr Tweets absetzt. Da wir die Anzahl an Tweets pro Tag betrachten möchten, erstellen wir eine neue Variable &lt;em&gt;date&lt;/em&gt;, die - anders als die Variable &lt;em&gt;created_at&lt;/em&gt; - nicht mehr die genaue Uhrzeit des Posts enthält.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trump_timeline &amp;lt;- trump_timeline %&amp;gt;% 
  mutate(date = as.Date(created_at)) 

biden_timeline &amp;lt;- biden_timeline %&amp;gt;% 
  mutate(date = as.Date(created_at))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Anschließend gruppieren wir den jeweiligen Datensatz gemäß der Variablen &lt;em&gt;date&lt;/em&gt;, zählen die Tweets und speichern die Werte als &lt;em&gt;trump_tweets_per_day&lt;/em&gt; und &lt;em&gt;biden_tweets_per_day&lt;/em&gt;. Wir vereinigen die beiden Datensätze über einen Inner Join. So erhalten wir einen Datensatz mit den drei Variablen &lt;em&gt;date&lt;/em&gt;, &lt;em&gt;n_trump&lt;/em&gt; und &lt;em&gt;n_biden&lt;/em&gt;. Um die Daten visualisieren zu können, überführen wir den Datensatz mit der Funktion &lt;em&gt;pivot_longer&lt;/em&gt; in ein &lt;em&gt;tidy format&lt;/em&gt;, bei dem jede Spalte einer Variablen und jede Zeile einer Observation entspricht. Anschließend erstellen wir mit &lt;em&gt;ggplot&lt;/em&gt; eine Grafik, die die Anzahl der täglichen Tweets je Kandidat über den betrachteten Zeitraum gegenüberstellt.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trump_tweets_per_day &amp;lt;- trump_timeline %&amp;gt;% 
  count(date) %&amp;gt;% 
  arrange(desc(n))

biden_tweets_per_day &amp;lt;- biden_timeline %&amp;gt;% 
  count(date) %&amp;gt;% 
  arrange(desc(n))

trump_tweets_per_day %&amp;gt;% 
  inner_join(biden_tweets_per_day, 
             by = &amp;quot;date&amp;quot;,
             suffix = c(&amp;quot;_trump&amp;quot;, &amp;quot;_biden&amp;quot;)) %&amp;gt;% 
  pivot_longer(n_trump:n_biden,
               names_to = &amp;quot;candidate&amp;quot;,
               values_to = &amp;quot;n_tweets&amp;quot;) %&amp;gt;% 
  mutate(candidate = str_remove_all(candidate, &amp;quot;n_&amp;quot;)) %&amp;gt;% 
  mutate(candidate = factor(candidate, levels = c(&amp;quot;trump&amp;quot;, &amp;quot;biden&amp;quot;))) %&amp;gt;% 
  ggplot(aes(date, n_tweets, color = candidate)) +
  geom_line(size = 0.7) +
  labs(x = &amp;quot;Date&amp;quot;, y = &amp;quot;Number of Tweets&amp;quot;, 
       title = &amp;quot;Vergleich der Twitteraktivität&amp;quot;,
       subtitle = &amp;quot;der Präsidentschaftskandidaten&amp;quot;) +
  scale_color_manual(values = c(&amp;quot;orangered&amp;quot;, &amp;quot;dodgerblue2&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-10-twitter-analyse-textanalyse-der-tweets-der-kandidaten-um-das-amt-des-pr%C3%A4sidenten-der-usa.de_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Dabei fällt auf, dass Donald Trump deutlich häufiger Tweets absetzt als Joe Biden. Schauen wir uns, an was der Grund dafür sein könnte.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prop_retweet &amp;lt;- trump_timeline %&amp;gt;% 
  bind_rows(biden_timeline) %&amp;gt;% 
  group_by(screen_name) %&amp;gt;% 
  summarize(prop_retweet = mean(is_retweet)) 

prop_retweet %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;screen_name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;prop_retweet&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;JoeBiden&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0412500&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;realDonaldTrump&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5297569&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prop_retweet_trump &amp;lt;- prop_retweet %&amp;gt;% 
  filter(screen_name == &amp;quot;realDonaldTrump&amp;quot;) %&amp;gt;% 
  select(prop_retweet) %&amp;gt;% 
  as_vector() %&amp;gt;% 
  scales::percent()

prop_retweet_biden &amp;lt;- prop_retweet %&amp;gt;% 
  filter(screen_name == &amp;quot;JoeBiden&amp;quot;) %&amp;gt;% 
  select(prop_retweet) %&amp;gt;% 
  as_vector() %&amp;gt;% 
  scales::percent()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ein Grund für die deutliche höhere Twitter Frequenz von Donald Trump ist der deutlich höhere Anteil an Retweets bei ihm. Während der Anteil an Retweets an den betrachteten Tweets von Joe Biden nur etwa 4% betragen, bestehen die Tweets von Donald Trump mit rund 53% fast zur Hälfte aus Retweets. Betrachetn wir, wie sich der Vergleich der Anzahl an täglichen Tweets verändert, wenn wir Retweets zuvor entfernen.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trump_timeline &amp;lt;- trump_timeline %&amp;gt;% 
  filter(!is_retweet)

biden_timeline &amp;lt;- biden_timeline %&amp;gt;% 
  filter(!is_retweet)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Auch ohne Retweets twittert Trump im Schnitt häufiger als Biden, allerdings ist der Unterschied deutlich geringer als zuvor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trump_tweets_per_day &amp;lt;- trump_timeline %&amp;gt;% 
  count(date) %&amp;gt;% 
  arrange(desc(n))

biden_tweets_per_day &amp;lt;- biden_timeline %&amp;gt;% 
  count(date) %&amp;gt;% 
  arrange(desc(n))

trump_tweets_per_day %&amp;gt;% 
  inner_join(biden_tweets_per_day, 
             by = &amp;quot;date&amp;quot;,
             suffix = c(&amp;quot;_trump&amp;quot;, &amp;quot;_biden&amp;quot;)) %&amp;gt;% 
  pivot_longer(n_trump:n_biden,
               names_to = &amp;quot;candidate&amp;quot;,
               values_to = &amp;quot;n_tweets&amp;quot;) %&amp;gt;% 
  mutate(candidate = str_remove_all(candidate, &amp;quot;n_&amp;quot;)) %&amp;gt;% 
  mutate(candidate = factor(candidate, levels = c(&amp;quot;trump&amp;quot;, &amp;quot;biden&amp;quot;))) %&amp;gt;%
  ggplot(aes(date, n_tweets, color = candidate)) +
  geom_line(size = 0.7) +
  labs(x = &amp;quot;Date&amp;quot;, y = &amp;quot;Number of Tweets&amp;quot;, 
       title = &amp;quot;Vergleich der Twitteraktivität - ohne Retweets&amp;quot;,
       subtitle = &amp;quot;der Präsidentschaftskandidaten&amp;quot;) +
  scale_color_manual(values = c(&amp;quot;orangered&amp;quot;, &amp;quot;dodgerblue2&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-10-twitter-analyse-textanalyse-der-tweets-der-kandidaten-um-das-amt-des-pr%C3%A4sidenten-der-usa.de_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reaktionen-auf-die-tweets-der-kandidaten&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reaktionen auf die Tweets der Kandidaten&lt;/h2&gt;
&lt;p&gt;Nun gehen wir der Frage nach, wie häufig ein Retweet der jeweiligen Person retweeted wird bzw. wie viele Retweets die Kandidaten im Durchschnitt pro Tag hervorrufen.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_retweet_count_trump &amp;lt;- trump_timeline %&amp;gt;% 
  summarise(mean_retweet_count = mean(retweet_count)) %&amp;gt;% 
  as_vector() %&amp;gt;% 
  round()
mean_retweet_count_biden &amp;lt;- biden_timeline %&amp;gt;% 
  summarise(mean_retweet_count = mean(retweet_count)) %&amp;gt;% 
  as_vector() %&amp;gt;% 
  round()
prop_diff_retweets &amp;lt;- (1 - mean_retweet_count_biden / mean_retweet_count_trump) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Donald Trumps Tweets erzielen im Durchschnitt 2.298910^{4}, die von Joe Biden 9359 und damit um etwa 59% weniger.&lt;/p&gt;
&lt;p&gt;Betrachten wir die durchschnittliche Anzahl an Retweets, die die Tweets der Kandidaten pro Tag auslösen. Nachdem wir für beide Kandidaten die durchschnittlichen täglichen Retweets berstimmt haben, verbinden wir beide Datensätze per Inner Join und bringen den vereinigten Datensatz in ein &lt;em&gt;tidy format&lt;/em&gt;, um die Zeitreihe anschließend zu grafisch darzustellen.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trump_mean_retweet_per_day &amp;lt;- trump_timeline %&amp;gt;% 
  select(screen_name, date, retweet_count) %&amp;gt;% 
  group_by(date) %&amp;gt;% 
  summarize(retweet_count = mean(retweet_count))


biden_mean_retweet_per_day &amp;lt;- biden_timeline %&amp;gt;% 
  select(screen_name, date, retweet_count) %&amp;gt;% 
  group_by(date) %&amp;gt;% 
  summarize(retweet_count = mean(retweet_count))

mean_retweet_comparison &amp;lt;- trump_mean_retweet_per_day %&amp;gt;% 
  inner_join(biden_mean_retweet_per_day, 
             by = &amp;quot;date&amp;quot;,
             suffix = c(&amp;quot;_trump&amp;quot;, &amp;quot;_biden&amp;quot;)) %&amp;gt;% 
  pivot_longer(retweet_count_trump:retweet_count_biden,
               names_to = &amp;quot;candidate&amp;quot;,
               values_to = &amp;quot;retweet_count&amp;quot;) %&amp;gt;% 
  mutate(candidate = str_remove_all(string = candidate, 
                                    pattern = &amp;quot;retweet_count_&amp;quot;)) %&amp;gt;% 
  mutate(candidate = factor(candidate, levels = c(&amp;quot;trump&amp;quot;, &amp;quot;biden&amp;quot;)))
  

mean_retweet_comparison %&amp;gt;% 
  ggplot(aes(date, retweet_count, color = candidate)) +
  geom_line(size = 0.7) +
  labs(x = &amp;quot;Date&amp;quot;, y = &amp;quot;&amp;quot;, 
       title = &amp;quot;Durchschnittliche Anzahl an Retweets&amp;quot;,
       subtitle = &amp;quot;die ein Tweet der Kandidaten auslöst&amp;quot;) +
  scale_color_manual(values = c(&amp;quot;orangered&amp;quot;, &amp;quot;dodgerblue2&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-10-twitter-analyse-textanalyse-der-tweets-der-kandidaten-um-das-amt-des-pr%C3%A4sidenten-der-usa.de_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In der Regel werden die Tweets des Präsidenten öfters retweetet als die von Joe Biden.
Allerdings geht aus der Grafik hervor, dass Joe Biden an einem bestimmten Tag ungewöhnlich viele Retweets erzielte.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_retweet_comparison %&amp;gt;%
  filter(candidate == &amp;quot;biden&amp;quot;) %&amp;gt;% 
  arrange(desc(retweet_count))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 70 x 3
##    date       candidate retweet_count
##    &amp;lt;date&amp;gt;     &amp;lt;fct&amp;gt;             &amp;lt;dbl&amp;gt;
##  1 2020-08-11 biden            46829.
##  2 2020-08-24 biden            24570.
##  3 2020-09-04 biden            21966.
##  4 2020-08-31 biden            21934.
##  5 2020-07-04 biden            21482.
##  6 2020-09-11 biden            21004 
##  7 2020-07-17 biden            20377.
##  8 2020-08-29 biden            20123 
##  9 2020-07-06 biden            19756.
## 10 2020-07-19 biden            19714.
## # … with 60 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;biden_timeline %&amp;gt;% 
  filter(date == &amp;quot;2020-08-11&amp;quot;) %&amp;gt;% 
  select(date, text, retweet_count) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   date       text                                                  retweet_count
##   &amp;lt;date&amp;gt;     &amp;lt;chr&amp;gt;                                                         &amp;lt;int&amp;gt;
## 1 2020-08-11 &amp;quot;Back when Kamala was Attorney General, she worked c…         34201
## 2 2020-08-11 &amp;quot;I have the great honor to announce that I’ve picked…        169980
## 3 2020-08-11 &amp;quot;Don&amp;#39;t forget that in the middle of this pandemic, t…         10235
## 4 2020-08-11 &amp;quot;I promise you if I&amp;#39;m elected, I won&amp;#39;t waste any tim…         41675
## 5 2020-08-11 &amp;quot;This is the latest dire consequence of Donald Trump…          7380
## 6 2020-08-11 &amp;quot;Social Security is the bedrock of American retireme…         17504&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Es stellt sich heraus, dass es sich bei diesem Tag um den 11.08.2020 handelt, den Tag, an dem Biden Kamala Harris als seine Kandidatin für das Amt des Vizepräsidenten bekanntgab.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trump_higher_retweet &amp;lt;- mean_retweet_comparison %&amp;gt;% 
  pivot_wider(names_from = candidate,
              values_from = retweet_count,
              names_prefix = &amp;quot;n_&amp;quot;) %&amp;gt;% 
  mutate(trump_higher = n_trump &amp;gt; n_biden) %&amp;gt;% 
  select_if(is.logical) %&amp;gt;% 
  colMeans() %&amp;gt;% 
  scales::percent()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Insegesamt über den betrachteten Zeitraum erzielten die Tweets von Trump aber an 84% der Tage mehr Retweets als die seines Herausforderers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;populärste-tweets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Populärste Tweets&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;biden_timeline %&amp;gt;% 
  filter(!is_retweet) %&amp;gt;% 
  select(text, retweet_count) %&amp;gt;% 
  arrange(desc(retweet_count)) %&amp;gt;% 
  head(n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##    text                                                            retweet_count
##    &amp;lt;chr&amp;gt;                                                                   &amp;lt;int&amp;gt;
##  1 &amp;quot;I can’t believe I have to say this, but please don’t drink bl…        329835
##  2 &amp;quot;I have the great honor to announce that I’ve picked @KamalaHa…        169980
##  3 &amp;quot;When Donald Trump says tonight you won’t be safe in Joe Biden…        110381
##  4 &amp;quot;Mr. President, if you don’t respect our troops, you can’t lea…         92171
##  5 &amp;quot;Nearly 100,000 lives have been lost, and tens of millions are…         85802
##  6 &amp;quot;Donald Trump is the worst possible person to lead us through …         79114
##  7 &amp;quot;We can’t take four more years of this.&amp;quot;                                78720
##  8 &amp;quot;You won&amp;#39;t have to worry about my tweets when I&amp;#39;m president.&amp;quot;           77708
##  9 &amp;quot;Wear a mask.&amp;quot;                                                          72816
## 10 &amp;quot;Remember: every example of violence Donald Trump decries has …         70079&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trump_timeline %&amp;gt;% 
  filter(!is_retweet) %&amp;gt;% 
  select(text, retweet_count) %&amp;gt;% 
  arrange(desc(retweet_count)) %&amp;gt;% 
  head(n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##    text                                                            retweet_count
##    &amp;lt;chr&amp;gt;                                                                   &amp;lt;int&amp;gt;
##  1 IF YOU CAN PROTEST IN PERSON, YOU CAN VOTE IN PERSON!                  138032
##  2 DRAIN THE SWAMP! https://t.co/68M4sN7LLD                               119166
##  3 China has caused great damage to the United States and the res…        109710
##  4 https://t.co/36b2xC1GZf                                                 94649
##  5 MAKE AMERICA GREAT AGAIN!                                               89206
##  6 FAKE NEWS IS THE ENEMY OF THE PEOPLE!                                   85418
##  7 We are going to WIN the 2020 Election, BIG! #MAGA                       83329
##  8 Don’t buy GOODYEAR TIRES - They announced a BAN ON MAGA HATS. …         83227
##  9 https://t.co/jXoffXyZed                                                 80250
## 10 OPEN THE SCHOOLS!!!                                                     78324&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;analyse-der-tweets-text-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Analyse der Tweets: Text Analysis&lt;/h2&gt;
&lt;p&gt;Bisher haben wir uns numerische Kennzahlen zu den Kandidaten angeschaut wie &lt;em&gt;Anzahl der Follower&lt;/em&gt;, &lt;em&gt;Anzahl an Tweets pro Tag&lt;/em&gt;, &lt;em&gt;Anzahl an Retweets pro Tweet&lt;/em&gt; und einen ersten Blick auf die populärsten Tweets der Kandidaten geweorfen. Als nächstes wollen wir uns näher mit dem Inhalt der Tweets auseinandersetzen. Dabei betrachten wir häufig verwendete Worte der Kandidaten, bestimmen die Emotionen, die die Kandidaten in ihren Tweets ausdrücken und untersuchen, ob sich die Emotionen der populärsten Tweets von denen aller Tweets des jeweiligen Bewerbers unterscheiden. Zuvor müssen wir die Daten aber für die Textanalyse vorbereiten.&lt;/p&gt;
&lt;div id=&#34;preparation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Preparation&lt;/h3&gt;
&lt;p&gt;Als erstes speichern wir die Tweets der Kandidaten jeweils in den Vektoren &lt;em&gt;trump_text&lt;/em&gt; bzw. &lt;em&gt;biden_text&lt;/em&gt; und entfernen Links und Sonderzeichen aus den Tweets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trump_text &amp;lt;- trump_timeline$text
biden_text &amp;lt;- biden_timeline$text

trump_text_clean &amp;lt;- trump_text %&amp;gt;% 
  rm_twitter_url() %&amp;gt;% 
  str_replace_all(&amp;quot;[^A-Za-z]&amp;quot;, &amp;quot; &amp;quot;) 

biden_text_clean &amp;lt;- biden_text %&amp;gt;% 
  rm_twitter_url() %&amp;gt;% 
  str_replace_all(&amp;quot;[^A-Za-z]&amp;quot;, &amp;quot; &amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Anschließend werden die Textvektoren in sogenannte &lt;em&gt;Corpora&lt;/em&gt; umgewandelt.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trump_corpus &amp;lt;- trump_text_clean %&amp;gt;% 
  VectorSource() %&amp;gt;% 
  Corpus()

biden_corpus &amp;lt;- biden_text_clean %&amp;gt;% 
  VectorSource() %&amp;gt;% 
  Corpus()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wir wandeln alle in den Tweets verwendeten Großbuchstaben in Kleinbuchstaben um. Dadurch wird verhindert, dass z.B. die Worte “Big” und “big” als zwei unterschiedliche Worte gewertet werden.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trump_corpus &amp;lt;- trump_corpus %&amp;gt;% 
  tm_map(str_to_lower)

biden_corpus &amp;lt;- biden_corpus %&amp;gt;% 
  tm_map(str_to_lower)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Die Tweets enthalten zu einem großen Teil sogenannte Stopwords wie z.B. “and” und “have”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stopwords(&amp;quot;en&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1] &amp;quot;i&amp;quot;          &amp;quot;me&amp;quot;         &amp;quot;my&amp;quot;         &amp;quot;myself&amp;quot;     &amp;quot;we&amp;quot;        
##   [6] &amp;quot;our&amp;quot;        &amp;quot;ours&amp;quot;       &amp;quot;ourselves&amp;quot;  &amp;quot;you&amp;quot;        &amp;quot;your&amp;quot;      
##  [11] &amp;quot;yours&amp;quot;      &amp;quot;yourself&amp;quot;   &amp;quot;yourselves&amp;quot; &amp;quot;he&amp;quot;         &amp;quot;him&amp;quot;       
##  [16] &amp;quot;his&amp;quot;        &amp;quot;himself&amp;quot;    &amp;quot;she&amp;quot;        &amp;quot;her&amp;quot;        &amp;quot;hers&amp;quot;      
##  [21] &amp;quot;herself&amp;quot;    &amp;quot;it&amp;quot;         &amp;quot;its&amp;quot;        &amp;quot;itself&amp;quot;     &amp;quot;they&amp;quot;      
##  [26] &amp;quot;them&amp;quot;       &amp;quot;their&amp;quot;      &amp;quot;theirs&amp;quot;     &amp;quot;themselves&amp;quot; &amp;quot;what&amp;quot;      
##  [31] &amp;quot;which&amp;quot;      &amp;quot;who&amp;quot;        &amp;quot;whom&amp;quot;       &amp;quot;this&amp;quot;       &amp;quot;that&amp;quot;      
##  [36] &amp;quot;these&amp;quot;      &amp;quot;those&amp;quot;      &amp;quot;am&amp;quot;         &amp;quot;is&amp;quot;         &amp;quot;are&amp;quot;       
##  [41] &amp;quot;was&amp;quot;        &amp;quot;were&amp;quot;       &amp;quot;be&amp;quot;         &amp;quot;been&amp;quot;       &amp;quot;being&amp;quot;     
##  [46] &amp;quot;have&amp;quot;       &amp;quot;has&amp;quot;        &amp;quot;had&amp;quot;        &amp;quot;having&amp;quot;     &amp;quot;do&amp;quot;        
##  [51] &amp;quot;does&amp;quot;       &amp;quot;did&amp;quot;        &amp;quot;doing&amp;quot;      &amp;quot;would&amp;quot;      &amp;quot;should&amp;quot;    
##  [56] &amp;quot;could&amp;quot;      &amp;quot;ought&amp;quot;      &amp;quot;i&amp;#39;m&amp;quot;        &amp;quot;you&amp;#39;re&amp;quot;     &amp;quot;he&amp;#39;s&amp;quot;      
##  [61] &amp;quot;she&amp;#39;s&amp;quot;      &amp;quot;it&amp;#39;s&amp;quot;       &amp;quot;we&amp;#39;re&amp;quot;      &amp;quot;they&amp;#39;re&amp;quot;    &amp;quot;i&amp;#39;ve&amp;quot;      
##  [66] &amp;quot;you&amp;#39;ve&amp;quot;     &amp;quot;we&amp;#39;ve&amp;quot;      &amp;quot;they&amp;#39;ve&amp;quot;    &amp;quot;i&amp;#39;d&amp;quot;        &amp;quot;you&amp;#39;d&amp;quot;     
##  [71] &amp;quot;he&amp;#39;d&amp;quot;       &amp;quot;she&amp;#39;d&amp;quot;      &amp;quot;we&amp;#39;d&amp;quot;       &amp;quot;they&amp;#39;d&amp;quot;     &amp;quot;i&amp;#39;ll&amp;quot;      
##  [76] &amp;quot;you&amp;#39;ll&amp;quot;     &amp;quot;he&amp;#39;ll&amp;quot;      &amp;quot;she&amp;#39;ll&amp;quot;     &amp;quot;we&amp;#39;ll&amp;quot;      &amp;quot;they&amp;#39;ll&amp;quot;   
##  [81] &amp;quot;isn&amp;#39;t&amp;quot;      &amp;quot;aren&amp;#39;t&amp;quot;     &amp;quot;wasn&amp;#39;t&amp;quot;     &amp;quot;weren&amp;#39;t&amp;quot;    &amp;quot;hasn&amp;#39;t&amp;quot;    
##  [86] &amp;quot;haven&amp;#39;t&amp;quot;    &amp;quot;hadn&amp;#39;t&amp;quot;     &amp;quot;doesn&amp;#39;t&amp;quot;    &amp;quot;don&amp;#39;t&amp;quot;      &amp;quot;didn&amp;#39;t&amp;quot;    
##  [91] &amp;quot;won&amp;#39;t&amp;quot;      &amp;quot;wouldn&amp;#39;t&amp;quot;   &amp;quot;shan&amp;#39;t&amp;quot;     &amp;quot;shouldn&amp;#39;t&amp;quot;  &amp;quot;can&amp;#39;t&amp;quot;     
##  [96] &amp;quot;cannot&amp;quot;     &amp;quot;couldn&amp;#39;t&amp;quot;   &amp;quot;mustn&amp;#39;t&amp;quot;    &amp;quot;let&amp;#39;s&amp;quot;      &amp;quot;that&amp;#39;s&amp;quot;    
## [101] &amp;quot;who&amp;#39;s&amp;quot;      &amp;quot;what&amp;#39;s&amp;quot;     &amp;quot;here&amp;#39;s&amp;quot;     &amp;quot;there&amp;#39;s&amp;quot;    &amp;quot;when&amp;#39;s&amp;quot;    
## [106] &amp;quot;where&amp;#39;s&amp;quot;    &amp;quot;why&amp;#39;s&amp;quot;      &amp;quot;how&amp;#39;s&amp;quot;      &amp;quot;a&amp;quot;          &amp;quot;an&amp;quot;        
## [111] &amp;quot;the&amp;quot;        &amp;quot;and&amp;quot;        &amp;quot;but&amp;quot;        &amp;quot;if&amp;quot;         &amp;quot;or&amp;quot;        
## [116] &amp;quot;because&amp;quot;    &amp;quot;as&amp;quot;         &amp;quot;until&amp;quot;      &amp;quot;while&amp;quot;      &amp;quot;of&amp;quot;        
## [121] &amp;quot;at&amp;quot;         &amp;quot;by&amp;quot;         &amp;quot;for&amp;quot;        &amp;quot;with&amp;quot;       &amp;quot;about&amp;quot;     
## [126] &amp;quot;against&amp;quot;    &amp;quot;between&amp;quot;    &amp;quot;into&amp;quot;       &amp;quot;through&amp;quot;    &amp;quot;during&amp;quot;    
## [131] &amp;quot;before&amp;quot;     &amp;quot;after&amp;quot;      &amp;quot;above&amp;quot;      &amp;quot;below&amp;quot;      &amp;quot;to&amp;quot;        
## [136] &amp;quot;from&amp;quot;       &amp;quot;up&amp;quot;         &amp;quot;down&amp;quot;       &amp;quot;in&amp;quot;         &amp;quot;out&amp;quot;       
## [141] &amp;quot;on&amp;quot;         &amp;quot;off&amp;quot;        &amp;quot;over&amp;quot;       &amp;quot;under&amp;quot;      &amp;quot;again&amp;quot;     
## [146] &amp;quot;further&amp;quot;    &amp;quot;then&amp;quot;       &amp;quot;once&amp;quot;       &amp;quot;here&amp;quot;       &amp;quot;there&amp;quot;     
## [151] &amp;quot;when&amp;quot;       &amp;quot;where&amp;quot;      &amp;quot;why&amp;quot;        &amp;quot;how&amp;quot;        &amp;quot;all&amp;quot;       
## [156] &amp;quot;any&amp;quot;        &amp;quot;both&amp;quot;       &amp;quot;each&amp;quot;       &amp;quot;few&amp;quot;        &amp;quot;more&amp;quot;      
## [161] &amp;quot;most&amp;quot;       &amp;quot;other&amp;quot;      &amp;quot;some&amp;quot;       &amp;quot;such&amp;quot;       &amp;quot;no&amp;quot;        
## [166] &amp;quot;nor&amp;quot;        &amp;quot;not&amp;quot;        &amp;quot;only&amp;quot;       &amp;quot;own&amp;quot;        &amp;quot;same&amp;quot;      
## [171] &amp;quot;so&amp;quot;         &amp;quot;than&amp;quot;       &amp;quot;too&amp;quot;        &amp;quot;very&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Diese Worte bieten für sich alleine betrachtet für die inhaltliche Analyse keinen Mehrwert und trüben stattdessen den Blick auf die wesentlichen, individuellen Worte der Kandidaten. Deshalb werden zunächst die gängigsten Stopwords entfernt.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trump_corpus &amp;lt;- trump_corpus %&amp;gt;% 
  tm_map(removeWords, stopwords(&amp;quot;en&amp;quot;))

biden_corpus &amp;lt;- biden_corpus %&amp;gt;% 
  tm_map(removeWords, stopwords(&amp;quot;en&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nach Entfernung der Sonderzeichen, der Umformung in Kleinbuchstaben und der Entfernung der Stopwords, sehen die einzelnen Tweets etwa so aus, wie diese zwei Beispiele:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trump_corpus[[3]]$content&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot; joe biden  elected  far left lunatics won t just  running failed dem cities  will  running  department  justice   department  homeland security    u s  supreme court   city  town  suburb will  safe   november  rd   vote will save america &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;biden_corpus[[3]]$content&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;donald trump  holding  rally  michigan today  also refusing  fully fund  national guard    frontline workers  covid     responsible  testing  distributing food  medical supplies  michiganders need  pandemic response   pep rally &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;häufig-verwendete-begriffe&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Häufig verwendete Begriffe&lt;/h3&gt;
&lt;p&gt;Als nächstes schauen wir, welche Worte die Kandidaten am häufigsten verwenden.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;term_count_trump &amp;lt;- freq_terms(trump_corpus, 50)
term_count_biden &amp;lt;- freq_terms(biden_corpus, 50)

head(term_count_trump, 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    WORD      FREQ
## 1  will       258
## 2  great      199
## 3  amp        140
## 4  s          129
## 5  t          110
## 6  people     103
## 7  biden       95
## 8  thank       89
## 9  news        87
## 10 fake        80
## 11 big         77
## 12 just        77
## 13 joe         74
## 14 now         74
## 15 never       71
## 16 democrats   63
## 17 new         61
## 18 even        59
## 19 left        59
## 20 many        56&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(term_count_biden, 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    WORD      FREQ
## 1  s         1103
## 2  trump      716
## 3  president  579
## 4  donald     512
## 5  t          493
## 6  can        443
## 7  will       432
## 8  need       387
## 9  re         315
## 10 nation     299
## 11 one        295
## 12 us         272
## 13 american   264
## 14 people     253
## 15 get        251
## 16 country    249
## 17 m          237
## 18 ll         236
## 19 every      222
## 20 make       222&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Es ist erkennbar, dass die Tweets immer noch Worte (und Buchstaben) enthalten, die für die inhaltliche Auswertung der Tweets störend sind. Aus diesem Grund werden weitere Stopwords entfernt. Die Auswahl, welche weiteren Worte zu entfernen sind, ist natürlich subjektiv. Dieser Subjektivität sollte man sich bewusst sein, um einem möglichen Bias bei der Auswahl entgegenwirken zu können.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;custom_stop &amp;lt;- c(&amp;quot;will&amp;quot;, &amp;quot;amp&amp;quot;, &amp;quot;s&amp;quot;, &amp;quot;t&amp;quot;, &amp;quot;just&amp;quot;, &amp;quot;now&amp;quot;, &amp;quot;new&amp;quot;, &amp;quot;never&amp;quot;, 
                 &amp;quot;complete&amp;quot;, &amp;quot;total&amp;quot;, &amp;quot;m&amp;quot;, &amp;quot;like&amp;quot;, &amp;quot;even&amp;quot;, &amp;quot;re&amp;quot;, &amp;quot;ll&amp;quot;, 
                 &amp;quot;ve&amp;quot;, &amp;quot;just&amp;quot;, &amp;quot;let&amp;quot;, &amp;quot;can&amp;quot;, &amp;quot;need&amp;quot;, &amp;quot;every&amp;quot;, &amp;quot;make&amp;quot;, 
                 &amp;quot;one&amp;quot;, &amp;quot;get&amp;quot;, &amp;quot;take&amp;quot;, &amp;quot;going&amp;quot;, &amp;quot;ob&amp;quot;, &amp;quot;isn&amp;quot;, &amp;quot;also&amp;quot;, 
                 &amp;quot;got&amp;quot;, &amp;quot;vets&amp;quot;, &amp;quot;made&amp;quot;, &amp;quot;across&amp;quot;, &amp;quot;second&amp;quot;, &amp;quot;first&amp;quot;, 
                 &amp;quot;many&amp;quot;, &amp;quot;want&amp;quot;, &amp;quot;day&amp;quot;, &amp;quot;time&amp;quot;, &amp;quot;today&amp;quot;, &amp;quot;much&amp;quot;, &amp;quot;done&amp;quot;, 
                 &amp;quot;way&amp;quot;, &amp;quot;back&amp;quot;, &amp;quot;years&amp;quot;, &amp;quot;must&amp;quot;, &amp;quot;nothing&amp;quot;, &amp;quot;ever&amp;quot;, 
                 &amp;quot;ensure&amp;quot;, &amp;quot;know&amp;quot;, &amp;quot;really&amp;quot;)

trump_corpus &amp;lt;- trump_corpus %&amp;gt;% 
  tm_map(removeWords, custom_stop)

biden_corpus &amp;lt;- biden_corpus %&amp;gt;% 
  tm_map(removeWords, custom_stop)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_freq_words &amp;lt;- 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Betrachten wir nun einen &lt;em&gt;Bar Plot&lt;/em&gt; der 15 häufigsten Wörter, die Joe Biden und Donald Trump jeweils in ihren Tweets verwenden.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;term_count_clean_trump &amp;lt;- freq_terms(trump_corpus, n_freq_words)

term_count_clean_trump %&amp;gt;% 
  ggplot(aes(reorder(WORD, FREQ), FREQ)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;) +
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;Häufigkeit&amp;quot;, 
       title = &amp;quot;Am häufigsten von Donald Trump verwendete Worte&amp;quot;,
       subtitle = &amp;quot;nach Entfernung von Stopwords&amp;quot;) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-10-twitter-analyse-textanalyse-der-tweets-der-kandidaten-um-das-amt-des-pr%C3%A4sidenten-der-usa.de_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;term_count_clean_biden &amp;lt;- freq_terms(biden_corpus, n_freq_words)

term_count_clean_biden %&amp;gt;% 
  ggplot(aes(reorder(WORD, FREQ), FREQ)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;) +
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;Häufigkeit&amp;quot;, 
       title = &amp;quot;Am häufigsten von Joe Biden verwendete Worte&amp;quot;,
       subtitle = &amp;quot;nach Entfernung von Stopwords&amp;quot;) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-10-twitter-analyse-textanalyse-der-tweets-der-kandidaten-um-das-amt-des-pr%C3%A4sidenten-der-usa.de_files/figure-html/unnamed-chunk-22-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Alternativ können wir die am häufigsten verwendeten Wörter noch kompakter in Form einer &lt;em&gt;Word Cloud&lt;/em&gt; darstellen, bei der die Größe des Wortes proportional zu der Häufigkeit seiner Verwendung steht.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trump_corpus %&amp;gt;%
  wordcloud::wordcloud(max.words = 70, 
                       colors = brewer.pal(6, &amp;quot;Dark2&amp;quot;), 
                       scale = c(3, 0.5), 
                       random.order = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-10-twitter-analyse-textanalyse-der-tweets-der-kandidaten-um-das-amt-des-pr%C3%A4sidenten-der-usa.de_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;biden_corpus %&amp;gt;%
  wordcloud::wordcloud(max.words = 70, 
                       colors = brewer.pal(6, &amp;quot;Dark2&amp;quot;), 
                       scale = c(3, 0.5), 
                       random.order = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-10-twitter-analyse-textanalyse-der-tweets-der-kandidaten-um-das-amt-des-pr%C3%A4sidenten-der-usa.de_files/figure-html/unnamed-chunk-23-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Trump verwendet am häufigsten das Wort “great”. Bei Biden sind es die beiden Worte “president” und “trump”, die am häufigsten verwendet werden. Es scheint so, als würde sich Joe Biden in seinen Tweets vor allem an Donald Trump abarbeiten, während umgekehrt Donald Trump seinen Herausforderer deutlich seltener in seinen Tweets erwähnt.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rank_crisis &amp;lt;- which(term_count_biden$WORD == &amp;quot;crisis&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Donald Trump verwendet deutlich häufiger positive Adjektive wie “great” und “good”. Das kann darin begründet liegen, dass Trump als Amtsinhaber bestrebt ist, seine bisherige Politik als Erfolg zu verkaufen, während Joe Biden bestrebt sein wird, eine gegenteilige Darstellung zu verbreiten. So ist das Wort “crisis” bei Joe Biden auf Rang 24 der meistverwendeten Worte, bei Donald Trump kommt es nicht einmal unter den 1000 meistverwendeten Worten.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;freq_terms(trump_corpus, 1000) %&amp;gt;% 
  filter(WORD == &amp;quot;crisis&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] WORD FREQ
## &amp;lt;0 rows&amp;gt; (or 0-length row.names)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;topic-modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Topic Modeling&lt;/h2&gt;
&lt;p&gt;Nachdem wir bereits die am häufigsten verwendeten Wörter betrachtet haben, versuchen wir nun aus den verwendeten Worten und deren gemeinsamen Auftreten, Themen der Tweets zu identifizieren.&lt;/p&gt;
&lt;div id=&#34;preparation-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Preparation&lt;/h3&gt;
&lt;p&gt;Zunächst müssen die Daten aber in ein für die &lt;em&gt;Topic Analysis&lt;/em&gt; passendes Format gebracht werden.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dtm_trump &amp;lt;- DocumentTermMatrix(trump_corpus)
inspect(dtm_trump)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;&amp;lt;DocumentTermMatrix (documents: 1122, terms: 3245)&amp;gt;&amp;gt;
## Non-/sparse entries: 11149/3629741
## Sparsity           : 100%
## Maximal term length: 21
## Weighting          : term frequency (tf)
## Sample             :
##      Terms
## Docs  biden big democrats fake great joe left news people thank
##   32      0   0         0    0     0   0    0    0      1     0
##   5       1   0         0    0     0   1    0    0      0     0
##   506     2   0         0    0     0   0    0    0      1     0
##   528     0   0         0    0     1   0    0    0      0     0
##   541     1   0         0    0     0   2    1    0      0     0
##   650     0   0         0    0     0   0    0    0      0     0
##   776     0   1         0    0     0   0    0    0      0     0
##   779     0   0         1    0     0   0    0    0      0     0
##   919     0   1         1    0     1   0    1    0      1     0
##   938     0   0         0    0     0   0    1    0      0     0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;row_totals_trump &amp;lt;- dtm_trump %&amp;gt;% 
  apply(1, sum)

dtm_trump &amp;lt;- dtm_trump[row_totals_trump &amp;gt; 0, ]


dtm_biden &amp;lt;- DocumentTermMatrix(biden_corpus)
inspect(dtm_biden)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;&amp;lt;DocumentTermMatrix (documents: 2301, terms: 5143)&amp;gt;&amp;gt;
## Non-/sparse entries: 32110/11801933
## Sparsity           : 100%
## Maximal term length: 25
## Weighting          : term frequency (tf)
## Sample             :
##       Terms
## Docs   america american country crisis donald nation people president together
##   1006       0        0       0      0      0      0      0         0        0
##   1050       0        0       0      0      0      0      0         0        0
##   1117       0        0       0      0      0      0      0         0        0
##   1313       0        0       0      0      0      0      0         0        0
##   1397       0        0       0      0      0      0      0         0        0
##   1545       0        0       0      1      0      0      0         0        0
##   1827       0        1       0      0      0      0      0         0        0
##   2168       0        0       0      0      0      0      0         0        0
##   397        0        0       0      0      0      0      0         0        0
##   787        0        0       0      0      0      0      0         0        0
##       Terms
## Docs   trump
##   1006     1
##   1050     0
##   1117     0
##   1313     0
##   1397     0
##   1545     0
##   1827     0
##   2168     0
##   397      0
##   787      1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;row_totals_biden &amp;lt;- dtm_biden %&amp;gt;% 
  apply(1, sum)

dtm_biden &amp;lt;- dtm_biden[row_totals_biden &amp;gt; 0, ]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;modellierung&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Modellierung&lt;/h3&gt;
&lt;p&gt;Anschließend wird die Latent Dirichlet Allocation (LDA) durchgeführt, um Themenfelder in den Tweets zu erkennen.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lda_5_trump &amp;lt;- LDA(dtm_trump, k = 6)
lda_5_biden &amp;lt;- LDA(dtm_biden, k = 6)

terms(lda_5_trump, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Topic 1       Topic 2  Topic 3    Topic 4     Topic 5  Topic 6  
## [1,] &amp;quot;great&amp;quot;       &amp;quot;thank&amp;quot;  &amp;quot;portland&amp;quot; &amp;quot;democrats&amp;quot; &amp;quot;biden&amp;quot;  &amp;quot;news&amp;quot;   
## [2,] &amp;quot;endorsement&amp;quot; &amp;quot;great&amp;quot;  &amp;quot;federal&amp;quot;  &amp;quot;russia&amp;quot;    &amp;quot;big&amp;quot;    &amp;quot;fake&amp;quot;   
## [3,] &amp;quot;election&amp;quot;    &amp;quot;people&amp;quot; &amp;quot;democrat&amp;quot; &amp;quot;country&amp;quot;   &amp;quot;joe&amp;quot;    &amp;quot;great&amp;quot;  
## [4,] &amp;quot;win&amp;quot;         &amp;quot;state&amp;quot;  &amp;quot;law&amp;quot;      &amp;quot;great&amp;quot;     &amp;quot;prices&amp;quot; &amp;quot;foxnews&amp;quot;
## [5,] &amp;quot;amendment&amp;quot;   &amp;quot;usdot&amp;quot;  &amp;quot;national&amp;quot; &amp;quot;history&amp;quot;   &amp;quot;drug&amp;quot;   &amp;quot;china&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;terms(lda_5_biden, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Topic 1     Topic 2     Topic 3     Topic 4    Topic 5  Topic 6    
## [1,] &amp;quot;president&amp;quot; &amp;quot;trump&amp;quot;     &amp;quot;nation&amp;quot;    &amp;quot;american&amp;quot; &amp;quot;trump&amp;quot;  &amp;quot;trump&amp;quot;    
## [2,] &amp;quot;donald&amp;quot;    &amp;quot;president&amp;quot; &amp;quot;president&amp;quot; &amp;quot;people&amp;quot;   &amp;quot;donald&amp;quot; &amp;quot;president&amp;quot;
## [3,] &amp;quot;country&amp;quot;   &amp;quot;donald&amp;quot;    &amp;quot;donald&amp;quot;    &amp;quot;together&amp;quot; &amp;quot;nation&amp;quot; &amp;quot;american&amp;quot; 
## [4,] &amp;quot;people&amp;quot;    &amp;quot;crisis&amp;quot;    &amp;quot;help&amp;quot;      &amp;quot;trump&amp;quot;    &amp;quot;white&amp;quot;  &amp;quot;america&amp;quot;  
## [5,] &amp;quot;trump&amp;quot;     &amp;quot;country&amp;quot;   &amp;quot;covid&amp;quot;     &amp;quot;covid&amp;quot;    &amp;quot;people&amp;quot; &amp;quot;nation&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sentiment Analysis&lt;/h2&gt;
&lt;p&gt;Bei der Sentiment Analysis geht es darum, die Emotionen, die in den Tweets ausgedrückt werden, zu identifizieren.&lt;/p&gt;
&lt;p&gt;Zunächst muss der jeweilige &lt;em&gt;Corpus&lt;/em&gt; in ein sogenanntes &lt;em&gt;Plain Text Document&lt;/em&gt; umgewandelt werden. Anschließend werden die Tweets in den Vektoren &lt;em&gt;trump_sentiment_text&lt;/em&gt; bzw. &lt;em&gt;biden_sentiment_text&lt;/em&gt; gespeichert.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trump_sentiment &amp;lt;- tm_map(trump_corpus, PlainTextDocument)
trump_sentiment_text &amp;lt;- trump_sentiment[[&amp;quot;content&amp;quot;]]$content

biden_sentiment &amp;lt;- tm_map(biden_corpus, PlainTextDocument)
biden_sentiment_text &amp;lt;- biden_sentiment[[&amp;quot;content&amp;quot;]]$content&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mit der Funktion &lt;em&gt;get_nrc_sentiment&lt;/em&gt; werden die in den Tweets verwendeten Worte gemäß dem &lt;em&gt;NRC sentiment dictionary&lt;/em&gt; in 10 Kategorien klassifiziert.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trump_sa &amp;lt;- get_nrc_sentiment(trump_sentiment_text)
head(trump_sa)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   anger anticipation disgust fear joy sadness surprise trust negative positive
## 1     1            1       0    1   2       1        1     2        1        2
## 2     2            0       1    2   0       2        0     0        2        1
## 3     2            2       0    1   3       1        1     4        1        5
## 4     1            0       1    2   0       1        0     0        2        1
## 5     2            1       1    1   0       3        0     1        4        2
## 6     1            1       0    0   1       0        1     3        0        5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;biden_sa &amp;lt;- get_nrc_sentiment(biden_sentiment_text)
head(biden_sa)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   anger anticipation disgust fear joy sadness surprise trust negative positive
## 1     0            0       0    0   0       0        1     1        0        1
## 2     1            0       0    1   1       0        0     3        1        3
## 3     0            1       0    3   1       1        1     5        1        6
## 4     0            0       0    0   0       0        0     1        1        0
## 5     3            2       2    4   2       3        2     4        5        3
## 6     1            0       1    0   1       1        0     2        1        4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Jede Zeile in den Datensätzen &lt;em&gt;trump_sa&lt;/em&gt; und &lt;em&gt;biden_sa&lt;/em&gt; entspricht einem Tweet. Für jeden Emotionstyp bzw. jede Valenz gibt es eine Spalte. Die Scores können dazu genutzt werden, zu veranschaulichen welchen Anteil die jeweilige Emotion bzw. das jeweilige Senntiment(positve vs. negative) in den betrachteten Tweets der Kandidaten hat.&lt;/p&gt;
&lt;div id=&#34;anteil-der-jeweiligen-emotionen-in-den-tweets-der-kandidaten&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Anteil der jeweiligen Emotionen in den Tweets der Kandidaten&lt;/h3&gt;
&lt;p&gt;Zunächst summieren wir die Scores für die 8 Emotionen spaltenweise auf. Die so erhaltenen Summen pro Emotion werden nun jeweils ins Verhältnis zu den gesamten Scores der 8 Emotionen gesetzt. Das machen wir für Trump und Biden.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trump_sa_score_prop_emo &amp;lt;- trump_sa %&amp;gt;% 
  select(1:8) %&amp;gt;% 
  colSums() %&amp;gt;% 
  prop.table() %&amp;gt;% 
  data.frame(.) %&amp;gt;% 
  rename(&amp;quot;prop&amp;quot; = &amp;quot;.&amp;quot;) %&amp;gt;% 
  bind_cols(sentiment = row.names(.))

biden_sa_score_prop_emo &amp;lt;- biden_sa %&amp;gt;% 
  select(1:8) %&amp;gt;% 
  colSums() %&amp;gt;% 
  prop.table() %&amp;gt;% 
  data.frame(.) %&amp;gt;% 
  rename(&amp;quot;prop&amp;quot; = &amp;quot;.&amp;quot;) %&amp;gt;% 
  bind_cols(sentiment = row.names(.))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Anschließend verbinden wir die Datensätze &lt;em&gt;trump_sa_score_prop_emo&lt;/em&gt; und &lt;em&gt;biden_sa_score_prop_emo&lt;/em&gt; per &lt;em&gt;Inner Join&lt;/em&gt; und bringen die Daten in eine Form, die es uns ermöglicht, die Daten zu visualisieren.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sa_score_prop_df &amp;lt;- trump_sa_score_prop_emo %&amp;gt;% 
  inner_join(biden_sa_score_prop_emo,
             by = &amp;quot;sentiment&amp;quot;,
             suffix = c(&amp;quot;_trump&amp;quot;, &amp;quot;_biden&amp;quot;)) %&amp;gt;% 
  pivot_longer(c(prop_trump, prop_biden),
               names_to = &amp;quot;candidate&amp;quot;,
               values_to = &amp;quot;prop&amp;quot;) %&amp;gt;% 
  mutate(candidate = str_replace_all(string = candidate,
                                     pattern = &amp;quot;prop_&amp;quot;,
                                     replacement = &amp;quot;&amp;quot;)) %&amp;gt;% 
  mutate(candidate = factor(candidate, levels = c(&amp;quot;trump&amp;quot;, &amp;quot;biden&amp;quot;))) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Die nachfolgende Grafik gibt für jede der Emotionen an, wie hoch ihr Anteil an den Tweets des jeweiligen Kandidaten für die Präsidentschaft ist.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sa_score_prop_df %&amp;gt;% 
  ggplot(aes(fct_reorder(sentiment, prop), prop, fill = candidate)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, position = &amp;quot;dodge&amp;quot;) +
  scale_fill_manual(values = c(&amp;quot;orangered&amp;quot;, &amp;quot;dodgerblue2&amp;quot;)) +
  labs(x = &amp;quot;Sentiment&amp;quot;, y = &amp;quot;&amp;quot;,
       title = &amp;quot;Anteil der Emotionen in den Tweets&amp;quot;,
       subtitle = &amp;quot;der jeweiligen Kandidaten&amp;quot;) +
  scale_y_continuous(labels = scales::percent)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-10-twitter-analyse-textanalyse-der-tweets-der-kandidaten-um-das-amt-des-pr%C3%A4sidenten-der-usa.de_files/figure-html/unnamed-chunk-33-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Biden liegt insebesondere bei den Emotionen &lt;em&gt;trust&lt;/em&gt; und &lt;em&gt;surprise&lt;/em&gt; vor Trump, während Trump bei den Emotionen &lt;em&gt;disgust&lt;/em&gt;, &lt;em&gt;sadness&lt;/em&gt;, &lt;em&gt;anger&lt;/em&gt; und &lt;em&gt;fear&lt;/em&gt; vorne liegt. Bei &lt;em&gt;joy&lt;/em&gt; und &lt;em&gt;anticipation&lt;/em&gt; liegen beide etwa gleichauf.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;anteil-der-positivität-vs.negativität&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Anteil der Positivität vs. Negativität&lt;/h3&gt;
&lt;p&gt;Neben den 8 Emotionen liefert die Sentiment Analysis bei Nutzung des NRC Lexikons auch Scores für die Kategorien &lt;em&gt;Positiv&lt;/em&gt; und &lt;em&gt;Negativ&lt;/em&gt;. Für diese Kategorien wiederholen wir die Schritte, die wir bei der Untersuchung der Emotionen durchgeführt haben analog.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trump_sa_score_prop_pos &amp;lt;- trump_sa %&amp;gt;% 
  select(negative, positive) %&amp;gt;%  # negative und positive statt der 8 Emotionen
  colSums() %&amp;gt;% 
  prop.table() %&amp;gt;% 
  data.frame(.) %&amp;gt;% 
  rename(&amp;quot;prop&amp;quot; = &amp;quot;.&amp;quot;) %&amp;gt;% 
  bind_cols(sentiment = row.names(.))

biden_sa_score_prop_pos &amp;lt;- biden_sa %&amp;gt;% 
  select(negative, positive) %&amp;gt;%  # negative und positive statt der 8 Emotionen
  colSums() %&amp;gt;% 
  prop.table() %&amp;gt;% 
  data.frame(.) %&amp;gt;% 
  rename(&amp;quot;prop&amp;quot; = &amp;quot;.&amp;quot;) %&amp;gt;% 
  bind_cols(sentiment = row.names(.))

sa_score_prop_pos_df &amp;lt;- trump_sa_score_prop_pos %&amp;gt;% 
  inner_join(biden_sa_score_prop_pos,
             by = &amp;quot;sentiment&amp;quot;,
             suffix = c(&amp;quot;_trump&amp;quot;, &amp;quot;_biden&amp;quot;)) %&amp;gt;% 
  pivot_longer(c(prop_trump, prop_biden),
               names_to = &amp;quot;candidate&amp;quot;,
               values_to = &amp;quot;prop&amp;quot;) %&amp;gt;% 
  mutate(candidate = str_replace_all(string = candidate,
                                     pattern = &amp;quot;prop_&amp;quot;,
                                     replacement = &amp;quot;&amp;quot;)) %&amp;gt;% 
  mutate(candidate = factor(candidate, levels = c(&amp;quot;trump&amp;quot;, &amp;quot;biden&amp;quot;))) 


sa_score_prop_pos_df %&amp;gt;% 
  ggplot(aes(fct_reorder(sentiment, prop), prop, fill = candidate)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, 
           position = &amp;quot;dodge&amp;quot;,
           width = 0.5) +
  facet_wrap(~candidate) +
  scale_fill_manual(values = c(&amp;quot;orangered&amp;quot;, &amp;quot;dodgerblue2&amp;quot;)) +
  labs(x = &amp;quot;Sentiment&amp;quot;, y = &amp;quot;&amp;quot;,
       title = &amp;quot;Anteil der Valenz in den Tweets&amp;quot;,
       subtitle = &amp;quot;der jeweiligen Kandidaten&amp;quot;) +
  scale_y_continuous(labels = scales::percent)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-10-twitter-analyse-textanalyse-der-tweets-der-kandidaten-um-das-amt-des-pr%C3%A4sidenten-der-usa.de_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Die Grafik zeigt, dass Bidens Tweets gemäß dem NRC Lexikon als deutlich positiver einzuordnen sind als die Tweets des Präsidenten.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;zusammenfassung&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Zusammenfassung&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Wir haben die Twitterhäufigkeit untersucht und festgestellt, dass Donald Trump häufiger twittert und deutlich häufiger retweetet als Joe Biden.&lt;/li&gt;
&lt;li&gt;Wir habe gesehen, dass Donald Trump deutlich mehr Follower hat und seine Tweets häufiger geretweetet werden.&lt;/li&gt;
&lt;li&gt;Wir haben die Worte visualisiert, die die Kandidaten am häufigsten verwenden.&lt;/li&gt;
&lt;li&gt;Es ist aufgefallen, dass Biden deutlich häufiger den Namen seines Kontrahenten verwendet als umgekehrt.&lt;/li&gt;
&lt;li&gt;Eine Sentiment Analyse ergab, dass Bidens Tweets insgesamt “positiver” sind als die von Trump.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Random Forest: Hyperparametertuning mit tidymodels im Rahmen der Titanic Machine Learning Competition auf Kaggle</title>
      <link>/post/random-forest-hyperparametertuning-mit-tidymodels-im-rahmen-der-kaggle-machine-learning-competition-titanic/</link>
      <pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/post/random-forest-hyperparametertuning-mit-tidymodels-im-rahmen-der-kaggle-machine-learning-competition-titanic/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Eine Analyse für die Teilnahme an der Titanic Machine Learning Competition auf &lt;a href=&#34;https://www.kaggle.com/c/titanic&#34; class=&#34;uri&#34;&gt;https://www.kaggle.com/c/titanic&lt;/a&gt;. Das Hauptaugenmerk dieser Analyse liegt auf dem Hyperparametertuning per Cross-Validation. Für die Modellierung wird &lt;em&gt;tidymodels&lt;/em&gt; -der Nachfolger von &lt;em&gt;caret&lt;/em&gt;- verwendet.&lt;/p&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Packages&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidymodels)
library(doParallel)
library(vip)
library(kableExtra)
library(modelr)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;get-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Get the Data&lt;/h2&gt;
&lt;p&gt;Im ersten Schritt laden wir den Trainingsdatensatz und den Testdatensatz, nachdem wir beide unter &lt;a href=&#34;https://www.kaggle.com/c/titanic/data&#34; class=&#34;uri&#34;&gt;https://www.kaggle.com/c/titanic/data&lt;/a&gt; heruntergeladen haben.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;titanic &amp;lt;- read_csv(&amp;quot;train.csv&amp;quot;)
titanic_holdout &amp;lt;- read_csv(&amp;quot;test.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploarative-datenanalyse&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploarative Datenanalyse&lt;/h2&gt;
&lt;p&gt;Wir beginnen mit einer kurzen explorativen Datenanalyse.&lt;/p&gt;
&lt;p&gt;Der Datensatz &lt;em&gt;titanic&lt;/em&gt; enthält Informationen aus 12 Variablen für 891 Passagiere der Titanic. Abzüglich der Variablen &lt;em&gt;Survived&lt;/em&gt; und &lt;em&gt;PassengerId&lt;/em&gt; stehen uns zunächst 10 Prädiktoren zur Verfügung.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(titanic)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 891
## Columns: 12
## $ PassengerId &amp;lt;dbl&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…
## $ Survived    &amp;lt;dbl&amp;gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, …
## $ Pclass      &amp;lt;dbl&amp;gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, …
## $ Name        &amp;lt;chr&amp;gt; &amp;quot;Braund, Mr. Owen Harris&amp;quot;, &amp;quot;Cumings, Mrs. John Bradley (F…
## $ Sex         &amp;lt;chr&amp;gt; &amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;male&amp;quot;, &amp;quot;male&amp;quot;, &amp;quot;ma…
## $ Age         &amp;lt;dbl&amp;gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14,…
## $ SibSp       &amp;lt;dbl&amp;gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, …
## $ Parch       &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, …
## $ Ticket      &amp;lt;chr&amp;gt; &amp;quot;A/5 21171&amp;quot;, &amp;quot;PC 17599&amp;quot;, &amp;quot;STON/O2. 3101282&amp;quot;, &amp;quot;113803&amp;quot;, &amp;quot;3…
## $ Fare        &amp;lt;dbl&amp;gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625…
## $ Cabin       &amp;lt;chr&amp;gt; NA, &amp;quot;C85&amp;quot;, NA, &amp;quot;C123&amp;quot;, NA, NA, &amp;quot;E46&amp;quot;, NA, NA, NA, &amp;quot;G6&amp;quot;, &amp;quot;…
## $ Embarked    &amp;lt;chr&amp;gt; &amp;quot;S&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;Q&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S…&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;beziehung-zwischen-den-prädiktoren-und-der-zu-erklärenden-variablen-survived&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Beziehung zwischen den Prädiktoren und der zu erklärenden Variablen &lt;em&gt;Survived&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;Als erstes betrachten wir in welcher Beziehung die einzelnen Prädiktoren mit der zu erklärenden Variablen &lt;em&gt;Survived&lt;/em&gt; stehen.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;titanic %&amp;gt;% 
  select(Age, Fare, Parch, SibSp, Survived) %&amp;gt;% 
  mutate(Survived = factor(Survived)) %&amp;gt;% 
  pivot_longer(-Survived, names_to = &amp;quot;variable&amp;quot;, values_to = &amp;quot;value&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = Survived, y = value, fill = Survived)) +
  geom_boxplot() +
  facet_wrap(~variable, scales = &amp;quot;free&amp;quot;) +
  theme(axis.title = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-02-random-forest-hyperparametertuning-mit-tidymodels-im-rahmen-der-kaggle-machine-learning-competition-titanic.de_files/figure-html/EDA%201-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Die Grafik zeigt inwieweit sich die Verteilungen der numerischen Varaiblen unterscheiden, je nachedm ob die jeweiligen Passagiere (Observationen) überlebt haben oder nicht. Zur Überprüfung der Unterschiede in Alter und Ticketpreis wurden je eine ANOVA durchgeführt.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Age and Survived

anova_age_survived &amp;lt;- anova(aov(Age ~ Survived, data = titanic)) 
p_anova_age_survived &amp;lt;- anova_age_survived$`Pr(&amp;gt;F)`[1]


# Fare and Survived

anova_fare_survived &amp;lt;- anova(aov(Fare ~ Survived, data = titanic))
p_anova_fare_survived &amp;lt;- anova_fare_survived$`Pr(&amp;gt;F)`[1]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Die Überlebenden waren zwar statistisch signifikant jünger (ANOVA: p-Wert = 3.91%). Allerdings beträgt der Unterschied nur 2.3 Jahre (28.3 vs. 30.6) Die Überlebenden besaßen aber deutlich höherpreisige Tickets als die Verstorbenen (48.4 vs. 22.1, ANOVA: p-Wert &amp;lt; 0.01).&lt;/p&gt;
&lt;p&gt;Die nachfolgende Grafik zeigt die Beziehung der Variablen &lt;em&gt;Embarked&lt;/em&gt;, &lt;em&gt;Pclass&lt;/em&gt; und &lt;em&gt;Sex&lt;/em&gt; mit der zu erklärenden Variablen &lt;em&gt;Survived&lt;/em&gt;. Auf der y-Achse sind die Überlebensraten abgetragen.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;titanic %&amp;gt;% 
  select(Embarked, Sex, Pclass, Survived) %&amp;gt;% 
  filter(Embarked != &amp;quot;NA&amp;quot;) %&amp;gt;% 
  mutate_at(vars(Embarked, Sex, Pclass), as.factor) %&amp;gt;% 
  pivot_longer(-Survived, names_to = &amp;quot;variable&amp;quot;, values_to = &amp;quot;value&amp;quot;) %&amp;gt;% 
  group_by(variable, value) %&amp;gt;% 
  summarise(mean = mean(Survived, na.rm = T)) %&amp;gt;% 
  mutate(variable = factor(variable)) %&amp;gt;% 
  ggplot(aes(value, mean)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, na.rm = T) +
  facet_grid(~variable, scale = &amp;quot;free_x&amp;quot;, space = &amp;quot;free&amp;quot;) +
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;Überlebensrate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-02-random-forest-hyperparametertuning-mit-tidymodels-im-rahmen-der-kaggle-machine-learning-competition-titanic.de_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Passagiere der ersten Klasse überlebten deutlich häufiger als die Passagiere der zweiten bzw. dritten Klasse. Am deutlichsten ist der Unterschied der Überlebensrate zwischen Männern und Frauen. Während Frauen zu 74.20% überlebten, waren es unter den männlichen Passagieren nur 18.89%.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kollinearität-der-prädiktoren&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Kollinearität der Prädiktoren&lt;/h3&gt;
&lt;p&gt;Wir werfen einen kurzen Blick auf die Kollinearität der Prädiktoren.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;titanic %&amp;gt;% 
  select(Age, Fare, Parch, SibSp, Pclass) %&amp;gt;% 
  na.omit() %&amp;gt;% 
  cor() %&amp;gt;% 
  corrplot::corrplot.mixed()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-02-random-forest-hyperparametertuning-mit-tidymodels-im-rahmen-der-kaggle-machine-learning-competition-titanic.de_files/figure-html/Collinearity%20of%20numeric%20variables-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Die numerischen Prädiktoren weisen keine erhöhten Korrelationen untereinander auf. Dennoch werden wir zur Sicherheit vor der Modellierung im Schritt &lt;em&gt;recipe&lt;/em&gt; einen Algorithmus auf die Daten anwenden, der Prädiktoren mit zu großer absoluter Korrelation mit anderen Prädiktoren entfernt.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;schiefe-der-verteilungen&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Schiefe der Verteilungen&lt;/h3&gt;
&lt;p&gt;Als nächstes betrachten wir die Verteilungen der Prädiktorvariablen. Zu schiefe Verteilungen können bei der Prognose zu Problemen führen.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;titanic %&amp;gt;% 
  select(Age, Fare, SibSp, Parch) %&amp;gt;%
  pivot_longer(1:4, names_to = &amp;quot;variable&amp;quot;, values_to = &amp;quot;value&amp;quot;) %&amp;gt;% 
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~variable, scales = &amp;quot;free&amp;quot;) +
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-02-random-forest-hyperparametertuning-mit-tidymodels-im-rahmen-der-kaggle-machine-learning-competition-titanic.de_files/figure-html/Distribution%20of%20Predictors:%20Skewness-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Insbesondere die Variable &lt;em&gt;Fare&lt;/em&gt; weist eine enorm rechtsschiefe Verteilung auf. Deshalb werden wir vor der Modellierung die Yeo-Johnson Transformation anwenden, um die Verteilung von &lt;em&gt;Fare&lt;/em&gt; symmetrischer zu machen.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;entfernung-von-variablen&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Entfernung von Variablen&lt;/h2&gt;
&lt;div id=&#34;overfitting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overfitting&lt;/h3&gt;
&lt;p&gt;Um ein Overfitting zu vermeiden, möchten wir Variablen, die den jeweiligen Passagier eindeutig oder nahezu eindeutig identifizieren, nicht zur Modellierung verwenden. Um diese Variablen herauszufinden, betrachten wir die Anzahl der “unique values” der Variablen des Typs &lt;em&gt;character&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;titanic %&amp;gt;% 
  select_if(is.character) %&amp;gt;% 
  map(unique) %&amp;gt;% 
  map_df(length) %&amp;gt;% 
  pivot_longer(Name:Embarked, names_to = &amp;quot;variable&amp;quot;, values_to = &amp;quot;# unique values&amp;quot;) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
variable
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
# unique values
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Name
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
891
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Sex
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Ticket
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
681
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Cabin
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
148
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Embarked
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Die Variablen mit Typ &lt;em&gt;character&lt;/em&gt; haben teilweise sehr viele verschiedene Werte. Mit den Variablen &lt;em&gt;Name&lt;/em&gt; und &lt;em&gt;Ticket&lt;/em&gt; sind die jeweiligen Passagiere nahezu eindeutig identifizierbar. Um ein Overfitting zu vermeiden, werden &lt;em&gt;Name&lt;/em&gt; und &lt;em&gt;Ticket&lt;/em&gt; deshalb nicht zur Modellierung benutzt.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;missings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Missings&lt;/h2&gt;
&lt;p&gt;Für die Schätzung (Imputation) fehlender Werte verwenden wir &lt;em&gt;Bagged Tree Imputation&lt;/em&gt;. Enthält eine Variable allerdings zu viele Missings, so macht selbst eine Schätzung der fehlenden Werte keinen Sinn mehr und die Variable sollte entfernt werden.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;na_prop_cabin &amp;lt;- titanic %&amp;gt;% 
  count(Cabin) %&amp;gt;% 
  mutate(prop = n / sum(n)) %&amp;gt;% 
  arrange(desc(prop)) %&amp;gt;% 
  slice_head(n = 1) %&amp;gt;% 
  select(prop) %&amp;gt;% 
  as_vector()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Die Variable &lt;em&gt;Cabin&lt;/em&gt; enthält zu 77.10% fehlende Werte (NAs). Aus diesem Grund wird auch diese Variable nicht zur Modellierung verwendet.&lt;/p&gt;
&lt;p&gt;Insegesamt werden also &lt;em&gt;Cabin&lt;/em&gt;, &lt;em&gt;Name&lt;/em&gt; und &lt;em&gt;Ticket&lt;/em&gt; aus dem Datensatz entfernt.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;titanic &amp;lt;- titanic %&amp;gt;%
  select(-Cabin, -Name, -Ticket)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Neben der bereits entfernten Variablen &lt;em&gt;Cabin&lt;/em&gt; weisen noch zwei weitere Variablen fehlende Werte auf.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(titanic)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PassengerId       Survived          Pclass          Sex           
##  Min.   :  1.0   Min.   :0.0000   Min.   :1.000   Length:891        
##  1st Qu.:223.5   1st Qu.:0.0000   1st Qu.:2.000   Class :character  
##  Median :446.0   Median :0.0000   Median :3.000   Mode  :character  
##  Mean   :446.0   Mean   :0.3838   Mean   :2.309                     
##  3rd Qu.:668.5   3rd Qu.:1.0000   3rd Qu.:3.000                     
##  Max.   :891.0   Max.   :1.0000   Max.   :3.000                     
##                                                                     
##       Age            SibSp           Parch             Fare       
##  Min.   : 0.42   Min.   :0.000   Min.   :0.0000   Min.   :  0.00  
##  1st Qu.:20.12   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:  7.91  
##  Median :28.00   Median :0.000   Median :0.0000   Median : 14.45  
##  Mean   :29.70   Mean   :0.523   Mean   :0.3816   Mean   : 32.20  
##  3rd Qu.:38.00   3rd Qu.:1.000   3rd Qu.:0.0000   3rd Qu.: 31.00  
##  Max.   :80.00   Max.   :8.000   Max.   :6.0000   Max.   :512.33  
##  NA&amp;#39;s   :177                                                      
##    Embarked        
##  Length:891        
##  Class :character  
##  Mode  :character  
##                    
##                    
##                    
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Die Variable &lt;em&gt;Age&lt;/em&gt; hat 177 Observationen mit fehlendem Wert. Bei Variablen des Typs &lt;em&gt;character&lt;/em&gt; sind fehlende Werte durch bloßen Aufruf der Funktion &lt;em&gt;summary()&lt;/em&gt; nicht erkennbar. Durch zweifachen Aufruf der Funktion &lt;em&gt;map()&lt;/em&gt; können alle verbleibenden Variablen mit fehlenden Werten identifiziert werden.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map(titanic, is.na) %&amp;gt;% 
  map_df(sum) %&amp;gt;% 
  pivot_longer(PassengerId:Embarked, names_to = &amp;quot;variable&amp;quot;, values_to = &amp;quot;missings&amp;quot;) %&amp;gt;% 
  filter(missings &amp;gt; 0) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
variable
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
missings
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Age
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
177
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Embarked
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Die Variable &lt;em&gt;Embarked&lt;/em&gt; besitzt 2 fehlende Werte. Im Schritt &lt;em&gt;recipe&lt;/em&gt; wird die Schätzung der fehlenden Werte der Variablen &lt;em&gt;Age&lt;/em&gt; und &lt;em&gt;Embarked&lt;/em&gt; mithilfe von &lt;em&gt;Bagged Trees&lt;/em&gt; anhand der Ausprägungen der anderen Variablen des jeweiligen Passagiers spezifiziert.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-split&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Split&lt;/h2&gt;
&lt;p&gt;Bevor wir mit der Modellierung beginnen, wird der Datensatz &lt;em&gt;titanic&lt;/em&gt; in den Trainingsdatensatz &lt;em&gt;titanic_train&lt;/em&gt; und den Testdatensatz &lt;em&gt;titanic_test&lt;/em&gt; aufgesplittet. Dabei wird &lt;em&gt;Stratified Sampling&lt;/em&gt; verwendet, um zu erreichen, dass die Verteilung von Überlebenden und Verstorbenen in beiden Datensätzen etwa gleich groß ist.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
titanic_split &amp;lt;- initial_split(titanic, strata = Survived)
titanic_train &amp;lt;- training(titanic_split)
titanic_test &amp;lt;- testing(titanic_split)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Insgesamt haben wir also drei Datensätze: Den Trainingsdatensatz &lt;em&gt;titanic_train&lt;/em&gt; an dem wir mit 10-fold Cross Validation die Werte der jeweiligen Hyperparameter der Modelle bestimmen. Der Testdatensatz &lt;em&gt;titanic_test&lt;/em&gt; andem wir die Performance des durch Cross Validation gefundenen besten Modells auf einem, dem Modell unbekannten, Datensatz testen. Und den Datensatz &lt;em&gt;titanic_holdout&lt;/em&gt;, bei dem wir nicht wissen, ob die Passagiere überlebt haben oder nicht und für den wir diese Werte im Rahmen der Kaggle Competition prognostizieren müssen.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modellierung&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modellierung&lt;/h2&gt;
&lt;p&gt;Die Modellierung mit &lt;em&gt;tidymodels&lt;/em&gt; kann in folgende Schritte unterteilt werden. Zunächst wird ein sogenanntes &lt;em&gt;Rezept&lt;/em&gt; erstellt, dass alle Pre-Processing Schritte wie Imputation und Transformationen von Variablen umfasst. Danach wird die sogenannte Modellspezifikation vorgenommen. Danach werden &lt;em&gt;Rezept&lt;/em&gt; und &lt;em&gt;Modellspezifikation&lt;/em&gt; im sogenannten &lt;em&gt;Workflow&lt;/em&gt; zusammengefasst. Dieser Workflow wird dann genutzt, um das Hyperparametertuning, die Validierung des besten Modells und die abschließende Prognose für die Competition zu erstellen.&lt;/p&gt;
&lt;div id=&#34;recipe&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Recipe&lt;/h3&gt;
&lt;p&gt;Zunächst wird mit der Funktion &lt;em&gt;recipe()&lt;/em&gt; festgelegt, welche Pre-Processing Schritte durchzuführen sind. Hier wird umgesetzt, was wir im Rahmen der explorativen Datenanalysen herausgefunden haben. &lt;em&gt;PClass&lt;/em&gt; und &lt;em&gt;Survived&lt;/em&gt; werden zu Faktorvariablen umgewandelt. Fehlende Werte der Variablen &lt;em&gt;Age&lt;/em&gt; und &lt;em&gt;Embarked&lt;/em&gt; werden per &lt;em&gt;Bagging&lt;/em&gt; geschätzt und auf die Variable &lt;em&gt;Fare&lt;/em&gt; wird die Yeo-Johnson Transformation angewendet.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;titanic_rec &amp;lt;- recipe(Survived ~ ., data = titanic_train) %&amp;gt;%
  update_role(PassengerId, new_role = &amp;quot;ID&amp;quot;) %&amp;gt;% 
  step_mutate(Pclass = factor(Pclass, labels = c(&amp;quot;first&amp;quot;, &amp;quot;second&amp;quot;, &amp;quot;third&amp;quot;)) %&amp;gt;% 
                fct_rev()) %&amp;gt;% 
  step_mutate(Survived = factor(Survived)) %&amp;gt;% 
  step_bagimpute(all_predictors()) %&amp;gt;%
  step_YeoJohnson(Fare)  %&amp;gt;% 
  #step_dummy(Pclass, Embarked, Sex) %&amp;gt;% 
  prep()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Um zu überprüfen, welche Auswirkung die Yeo-Johnson Transformation auf die Verteilung der Variablen &lt;em&gt;Fare&lt;/em&gt; hat, betrachten wir folgende Grafik. Die Verteilung ist immer noch rechtsschief, aber die Schiefe wurde deutlich reduziert.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bake(titanic_rec, new_data = titanic) %&amp;gt;% 
  select(Fare) %&amp;gt;% 
  mutate(Fare_original = titanic$Fare) %&amp;gt;% 
  rename(Fare_transformed = Fare) %&amp;gt;% 
  pivot_longer(1:2, names_to = &amp;quot;variable&amp;quot;, values_to = &amp;quot;values&amp;quot;) %&amp;gt;% 
  mutate(variable = factor(variable)) %&amp;gt;% 
  ggplot(aes(x = values, fill = variable)) +
  geom_histogram() +
  facet_grid(~variable, scales = &amp;quot;free&amp;quot;) +
  theme(legend.position = &amp;quot;none&amp;quot;) %&amp;gt;% 
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-02-random-forest-hyperparametertuning-mit-tidymodels-im-rahmen-der-kaggle-machine-learning-competition-titanic.de_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modellspezifikation-und-workflow&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Modellspezifikation und Workflow&lt;/h3&gt;
&lt;p&gt;Im nächsten Schritt wird das Modell spezifiziert. Wir wählen ein Random Forest Modell zur Klassifizierung, welches als Engine das Paket &lt;em&gt;ranger&lt;/em&gt; verwendet. Die Hyperparameter &lt;em&gt;mtry&lt;/em&gt;, &lt;em&gt;trees&lt;/em&gt; und &lt;em&gt;min_n&lt;/em&gt; erhalten Platzhalter, da sie erst noch im nächsten Schritt per Cross Validation bestimmt werden.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_spec &amp;lt;- rand_forest(
  mode = &amp;quot;classification&amp;quot;,
  mtry = tune(),
  trees = tune(),
  min_n = tune()) %&amp;gt;% 
  set_engine(&amp;quot;ranger&amp;quot;, importance = &amp;quot;impurity&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Das angefertigte Rezept und die Modellspezifikation werden im sogenannten Workflow zusammengefasst.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_wflow &amp;lt;- workflow() %&amp;gt;% 
  add_recipe(titanic_rec) %&amp;gt;% 
  add_model(rf_spec)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;hyperparametertuning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Hyperparametertuning&lt;/h3&gt;
&lt;p&gt;Zur Durchführung des Hyperparametertuning werden 10 Folds aus dem Trainingsdatensatz gebildet, die jeweils in Analyseset und Assessmentset unterteilt sind. Je Fold wird das Analyseset zum Trainng des Modells genutzt, während anhand des Assessmentsets die Performance bestimmt wird. Auch hier nutzen wir &lt;em&gt;Stratified Sampling&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
titanic_train_folds &amp;lt;- vfold_cv(titanic_train, strata = Survived)
titanic_train_folds &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #  10-fold cross-validation using stratification 
## # A tibble: 10 x 2
##    splits           id    
##    &amp;lt;list&amp;gt;           &amp;lt;chr&amp;gt; 
##  1 &amp;lt;split [601/68]&amp;gt; Fold01
##  2 &amp;lt;split [601/68]&amp;gt; Fold02
##  3 &amp;lt;split [602/67]&amp;gt; Fold03
##  4 &amp;lt;split [602/67]&amp;gt; Fold04
##  5 &amp;lt;split [602/67]&amp;gt; Fold05
##  6 &amp;lt;split [602/67]&amp;gt; Fold06
##  7 &amp;lt;split [602/67]&amp;gt; Fold07
##  8 &amp;lt;split [603/66]&amp;gt; Fold08
##  9 &amp;lt;split [603/66]&amp;gt; Fold09
## 10 &amp;lt;split [603/66]&amp;gt; Fold10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wir bilden ein Grid, dass alle Kombinationen aus den Werten der Hyperparameter enthält. Wir versuchen 7 verschiedene Werte für die Anzahl der pro Split des jeweiligen Decision Trees verwendeten Prädiktoren (&lt;em&gt;mtry&lt;/em&gt;), 6 verschiedene Werte für die Anzahl der pro Random Forest verwendeten Decision Trees (&lt;em&gt;trees&lt;/em&gt;) und 8 verschiedene Werte für die minimale Anzahl an Observationen, die in einem Knoten eines Decision Trees notwendig ist, um einen Split dieses Knotens in Erwägung zu ziehen. Die Spanne der betrachteten Werte für &lt;em&gt;mtry&lt;/em&gt; wird auf 1 bis 7 festgelegt.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_param &amp;lt;- parameters(rf_spec) %&amp;gt;% 
  update(mtry = mtry(range = c(1, 7)),
         trees = trees(range = c(1, 2500))) %&amp;gt;% 
  finalize(x = titanic_train %&amp;gt;% select(-Survived))

grid_rf &amp;lt;- grid_regular(rf_param, levels = c(mtry = 7, trees = 6, min_n = 8))

grid_rf %&amp;gt;% 
  knitr::kable() %&amp;gt;% 
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;)) %&amp;gt;% 
  scroll_box(width = &amp;quot;35%&amp;quot;, height = &amp;quot;300px&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:300px; overflow-x: scroll; width:35%; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
mtry
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
trees
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
min_n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Insgesamt erhalten wir 336 Kombinationsmöglichkeiten für die Hyperparameter. Für jede dieser Kombinationen wird pro Fold ein Random Forest mit jeweils bis zu 2500 Decision Trees trainiert. Das ergibt insgesamt 3360 Random Forests. Um die Zeit zu reduzieren, die für das Trainieren all dieser Random Forests bzw. Decision Trees notwendig ist, verwenden wir &lt;em&gt;Parallel Processing&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_cores &amp;lt;- parallel::detectCores(logical = FALSE) 
doParallel::registerDoParallel(cores = all_cores)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In diesem Schritt findet das Training der 3360 Random Forests im Rahmen des Hyperparametertunings statt. Dazu wird der bereits definierte Workflow zusammen mit dem bereits spezifizierten Grid der zu testenden Parameterwerte und den gebildeten Folds in der Funktion &lt;em&gt;tune_grid&lt;/em&gt; verwendet.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tune_rf &amp;lt;- tune_grid(
  rf_wflow,
  resamples = titanic_train_folds,
  grid = grid_rf,
  control = control_grid(save_pred = T)
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualisierung&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualisierung&lt;/h3&gt;
&lt;p&gt;Nachdem die Modelle trainiert wurden, visualisieren wir die Ergebnisse der Cross Validation.&lt;/p&gt;
&lt;div id=&#34;accuracy-und-auc&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Accuracy und AUC&lt;/h4&gt;
&lt;p&gt;Als mögliche Werte für die Anzahl der verwendeten Trees pro Random Forest wurden die Werte 1, 500, 1000, 1500, 2000 und 2500 verwendet.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tune_rf %&amp;gt;% 
  collect_metrics() %&amp;gt;% 
  filter(.metric == &amp;quot;accuracy&amp;quot;) %&amp;gt;% 
  mutate(mtry = factor(mtry)) %&amp;gt;% 
  ggplot(aes(x = min_n, y = mean, color = mtry)) +
  geom_line() +
  geom_point() +
  facet_wrap(~trees, labeller = &amp;quot;label_both&amp;quot;) +
  labs(x = &amp;quot;Minimal Node Size (min_n)&amp;quot;,
       y = &amp;quot;Accuracy&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-02-random-forest-hyperparametertuning-mit-tidymodels-im-rahmen-der-kaggle-machine-learning-competition-titanic.de_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Die Grafik zeigt, dass -wie zu erwarten- die am schlechtesten performenden Modelle solche sind, die nur aus einem Tree bestehen. Ob 500, 1000, 15000, 2000 oder 2500 Trees pro Random Forest verwendet werden, scheint keinen Unterschied zu machen. Die Modelle mit einer &lt;em&gt;Minimal Node Size&lt;/em&gt; (&lt;em&gt;min_n&lt;/em&gt;) von etwa 20 bis 25 scheinen die besten Ergebnisse für die Metrik &lt;em&gt;Accuracy&lt;/em&gt; zu erzielen. Für die Anzahl der pro Knoten betrachteten Prädiktoren scheint ein Wert von etwa 3 gut zu funktionieren, wenn man die Kurven für die Accuracy betrachtet. Dieser Eindruck wird durch die nachfolgende Grafik bestätigt.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tune_rf %&amp;gt;% 
  collect_metrics() %&amp;gt;%
  filter(.metric == &amp;quot;accuracy&amp;quot; &amp;amp; trees != 1) %&amp;gt;% 
  select(mean, mtry:min_n) %&amp;gt;% 
  pivot_longer(mtry:min_n, 
               names_to = &amp;quot;parameter&amp;quot;,
               values_to = &amp;quot;value&amp;quot;) %&amp;gt;% 
  ggplot(aes(value, mean, color = parameter)) +
  geom_point() +
  geom_smooth() +
  facet_wrap(~parameter, scales = &amp;quot;free_x&amp;quot;) +
  labs(x = &amp;quot;&amp;quot;,
       y = &amp;quot;Accuracy&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-02-random-forest-hyperparametertuning-mit-tidymodels-im-rahmen-der-kaggle-machine-learning-competition-titanic.de_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;roc&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;ROC&lt;/h4&gt;
&lt;p&gt;Wir visualisieren die ROC Kurven, wobei wir jeweils die Kurven durch eine der drei Hyperparameter gruppieren und einfärben. Für die Metrik &lt;em&gt;Area under the Curve (AUC)&lt;/em&gt; spielt die Wahl der Hyperparameter insgesamt keine große Rolle, solange man mindestens 500 Trees pro Random Forest verwendet.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tune_rf %&amp;gt;% 
  collect_predictions() %&amp;gt;% 
  mutate(mtry = factor(mtry)) %&amp;gt;% 
  group_by(mtry) %&amp;gt;% 
  roc_curve(truth = Survived, .pred_1, event_level = &amp;quot;second&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = 1- specificity, y = sensitivity, color = mtry)) +
  geom_path() +
  geom_abline(lty = 3, alpha = 0.6) +
  coord_equal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-02-random-forest-hyperparametertuning-mit-tidymodels-im-rahmen-der-kaggle-machine-learning-competition-titanic.de_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tune_rf %&amp;gt;% 
  collect_predictions() %&amp;gt;% 
  mutate(min_n = factor(min_n)) %&amp;gt;% 
  group_by(min_n) %&amp;gt;% 
  roc_curve(truth = Survived, .pred_1, event_level = &amp;quot;second&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = 1- specificity, y = sensitivity, color = min_n)) +
  geom_path() +
  coord_equal() +
  geom_abline(lty = 3, alpha = 0.6) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-02-random-forest-hyperparametertuning-mit-tidymodels-im-rahmen-der-kaggle-machine-learning-competition-titanic.de_files/figure-html/unnamed-chunk-18-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tune_rf %&amp;gt;% 
  collect_predictions() %&amp;gt;% 
  mutate(trees = factor(trees)) %&amp;gt;% 
  group_by(trees) %&amp;gt;% 
  roc_curve(truth = Survived, .pred_1, event_level = &amp;quot;second&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = 1- specificity, y = sensitivity, color = trees)) +
  geom_path() +
  coord_equal() +
  geom_abline(lty = 3, alpha = 0.6) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-02-random-forest-hyperparametertuning-mit-tidymodels-im-rahmen-der-kaggle-machine-learning-competition-titanic.de_files/figure-html/unnamed-chunk-18-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Was den Verlauf der &lt;em&gt;ROC Kurve&lt;/em&gt; betrifft, so hat &lt;em&gt;mtry&lt;/em&gt; keinen wesentlichen Einfluss, solange mtry größer als 1 ist. Die Roc Kurven unterschieden sich für verschiedene Werte von &lt;em&gt;min_n&lt;/em&gt; ebenfalls nur geringfügig. Die Anzahl der Bäume pro Random Forest spielt für den Verlauf der ROC Kurve keine Rolle, solange die Anzahl mindestens 500 beträgt.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;auswahl-der-werte-der-hyperparameter&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Auswahl der Werte der Hyperparameter&lt;/h3&gt;
&lt;p&gt;Nachdem wir visuell untersucht haben, welche Hyperparameterwerte sinnvoll erscheinen, wählen wir nun die endgültigen Werte der Hyperparameter. Dazu verwenden wir als Metriken sowohl die durchschnittliche Accuracy über alle 10 Folds, als auch den Standardfehler der 10 Accuracy Werte pro Parameterkombination. Zwar möchten wir ein Modell mit möglichst hoher durchschnittlicher Accuracy, allerdings ist es auch wichtig, eine Parameterkombination zu wählen, die bei Anwendung auf verschiedenen Datensätzen keine allzu schwankende Performance liefert. Dies ist insbesondere unter dem Aspekt wichtig, dass wie bei der Kaggle Competition Prognosen für nur einen Datensatz liefern müssen bzw. können und nicht wie bei der Cross Validation sozusagen 10 Versuche pro Parameterkombination haben, wobei nur der Durchschnittswert zählt. Dazu speichern wir jeweils die Parameterwerte der gemäß &lt;em&gt;Accuracy&lt;/em&gt; bzw. &lt;em&gt;Standardfehler der Accuracywerte&lt;/em&gt; 20 besten Modelle.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_rf &amp;lt;-tune_rf %&amp;gt;% 
  collect_metrics()

best_accuracy &amp;lt;- tune_rf %&amp;gt;% 
  show_best(metric = &amp;quot;accuracy&amp;quot;, n = 20) %&amp;gt;% 
  arrange(std_err)

best_accuracy %&amp;gt;% 
  knitr::kable() %&amp;gt;% 
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;),
                font_size = 13) %&amp;gt;% 
  scroll_box(height = &amp;quot;300px&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:300px; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;font-size: 13px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
mtry
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
trees
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
min_n
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
.metric
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
.estimator
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
mean
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
n
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
std_err
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
.config
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8355795
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0113953
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model207
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8326383
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0116948
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model193
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8341308
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0118343
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model179
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8340430
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0121825
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model220
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8340430
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0121825
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model234
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8326389
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0123361
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model165
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8326383
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0124897
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model200
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8385645
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0127936
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model178
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8370720
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0128932
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model185
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8370494
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0129054
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model157
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8340863
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0130965
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model164
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8340430
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0131592
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model241
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8326157
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0133199
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model108
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8385645
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0133398
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model136
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8355795
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0133708
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model143
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8385645
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0135240
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model192
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8385645
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0135240
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model206
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8356014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0137483
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model023
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8400351
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0139487
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model199
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8356014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0154207
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model150
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Die gemäß &lt;em&gt;Accuracy&lt;/em&gt; besten Modelle weisen eine über alle 10 Folds gemittelte Accuracy zwischen 83% und 84% auf. Die Darstellung erfolgt nach Streuung (std_err) sortiert.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_std &amp;lt;- cv_rf %&amp;gt;% 
  filter(.metric == &amp;quot;accuracy&amp;quot;) %&amp;gt;% 
  arrange(std_err) %&amp;gt;% 
  head(n = 20) 
best_std %&amp;gt;% 
  knitr::kable() %&amp;gt;% 
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;),
                font_size = 13) %&amp;gt;% 
  scroll_box(height = &amp;quot;300px&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:300px; &#34;&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;font-size: 13px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
mtry
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
trees
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
min_n
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
.metric
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
.estimator
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
mean
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
n
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
std_err
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
.config
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8071075
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0058506
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model336
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8086000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0067809
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model322
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8086000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0067809
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model329
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8071075
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0070058
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model315
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8100933
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0072176
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model308
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8116969
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0073757
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model252
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8145935
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0075619
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model307
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8100933
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0078641
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model314
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8087118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0078841
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model280
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8131009
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0078911
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model328
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8176012
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0080407
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model313
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8116749
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0080650
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model273
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8116084
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0081770
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model321
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8102043
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0082809
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model266
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8220794
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0082810
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model274
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8190491
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0083412
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model306
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8177561
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0086040
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model188
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8131675
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0086765
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model287
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8177122
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0087017
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model195
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8131236
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0087449
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Model335
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_params &amp;lt;- best_accuracy %&amp;gt;% 
  slice_head(n = 1) %&amp;gt;% 
  select(mtry, trees, min_n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bei den gemäß &lt;em&gt;Standardfehler der Accuracywerte&lt;/em&gt; besten Modellen beträgt die durchschnittliche Accuracy zwischen 80% und 82%. Als endgültige Parameter wählen wir die Werte des Modells, das unter den gemäß Accuracy 20 besten Modellen die geringste Streuung aufweist. Dieses Modell erzielte bei der Cross Validation eine deutlich überdurchschnittliche Accuracy und gleichzeitig eine unterdurchschnittliche Streuung. Der jeweilige Wert des gewählten Modells ist in den folgenden Boxplots durch einen roten Punkt gekennzeichnet.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_params_metrics &amp;lt;- best_accuracy %&amp;gt;% 
  slice_head(n = 1) %&amp;gt;% 
  select(Accuracy = mean, Streuung = std_err)

ref_point &amp;lt;- best_params_metrics %&amp;gt;% 
  pivot_longer(Accuracy:Streuung, names_to = &amp;quot;metric&amp;quot;, values_to = &amp;quot;values&amp;quot;)

cv_rf %&amp;gt;% 
  filter(.metric == &amp;quot;accuracy&amp;quot;) %&amp;gt;% 
  select(Accuracy = mean, Streuung = std_err) %&amp;gt;% 
  pivot_longer(Accuracy:Streuung, names_to = &amp;quot;metric&amp;quot;, values_to = &amp;quot;value&amp;quot;) %&amp;gt;% 
  ggplot(aes(value)) +
  geom_boxplot() +
  geom_point(data = ref_point, aes(x = values, y = 0), color = &amp;quot;red&amp;quot;, size = 3) +
  facet_wrap(~metric, scales = &amp;quot;free&amp;quot;) +
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-02-random-forest-hyperparametertuning-mit-tidymodels-im-rahmen-der-kaggle-machine-learning-competition-titanic.de_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Somit verwenden wir sowohl zur Validierung am Testdatensatz als auch zur Prognose am Datensatz &lt;em&gt;titanic_holdout&lt;/em&gt; die Parameterwerte:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mtry = 4&lt;/li&gt;
&lt;li&gt;trees = 2500&lt;/li&gt;
&lt;li&gt;min_n = 23&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Abschließend testen wir das Modell an dem Testdatensatz &lt;em&gt;titanic_test&lt;/em&gt;, der nicht zur &lt;em&gt;Model Selection&lt;/em&gt; verwendet wurde. Dieser Test dient dazu, eine realistische Einschätzung bezüglich der Performance unseres Modells an neuen Daten zu erhalten.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)

rf_fit_split &amp;lt;- rf_wflow %&amp;gt;%
  finalize_workflow(parameters = best_params) %&amp;gt;%
  last_fit(split = titanic_split)

test_metrics &amp;lt;- rf_fit_split %&amp;gt;%
  collect_metrics() 

test_metrics %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
.metric
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
.estimator
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
.estimate
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
accuracy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8288288
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
roc_auc
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8823100
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Bei der Prognose an dem Testdatensatz &lt;em&gt;titanic_test&lt;/em&gt; erzielt das Modell eine Accuracy von 82.9%.&lt;/p&gt;
&lt;p&gt;Die wichtigsten Prädiktoren unseres Modells können mit der Funktion &lt;em&gt;vip()&lt;/em&gt; visualisiert werden.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_fit_split %&amp;gt;%
  pluck(&amp;quot;.workflow&amp;quot;, 1) %&amp;gt;%
  pull_workflow_fit() %&amp;gt;%
  vip(num_features = 7)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-09-02-random-forest-hyperparametertuning-mit-tidymodels-im-rahmen-der-kaggle-machine-learning-competition-titanic.de_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Die wichtigste Variable ist mit Abstand das Geschlecht, gefolgt von der Höhe des Ticketpreises und dem Alter.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;kaggle-submission&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Kaggle Submission&lt;/h2&gt;
&lt;p&gt;Abschließend erstellen wir die &lt;em&gt;Prediction&lt;/em&gt; für die Submission auf Kaggle.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;last_rf_fit &amp;lt;- rf_wflow %&amp;gt;%
  finalize_workflow(parameters = best_params) %&amp;gt;%
  fit(data = titanic)

titanic_holdout &amp;lt;- titanic_holdout %&amp;gt;% 
  mutate(Survived = NA)

final_pred &amp;lt;- last_rf_fit %&amp;gt;% 
  extract_model() %&amp;gt;% 
  predict(bake(titanic_rec, titanic_holdout), type = &amp;quot;response&amp;quot;) 
  
final_pred_binary &amp;lt;- 
  ifelse(final_pred$predictions[,2] &amp;gt; 0.5, 1, 0) %&amp;gt;% 
  factor(levels = c(0, 1))

submission_df &amp;lt;- data.frame(PassengerId = titanic_holdout$PassengerId,
                            Survived = final_pred_binary)

write_csv(submission_df, &amp;quot;submission_kaggle.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Das Modell erzielt bei der Competition eine Accuracy von 79.2%. Damit liegt das Modell unter den Top 8% der Submissions.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Boosting: Implementierung von Hyperparameteruning per Cross Validation durch for-Schleifen</title>
      <link>/post/hyperparametertuning-mit-cross-validation/</link>
      <pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate>
      <guid>/post/hyperparametertuning-mit-cross-validation/</guid>
      <description>


&lt;p&gt;Der Fokus dieser Analyse liegt auf dem Hyperparametertuning per Cross-Validation. Insbesondere habe ich die Cross Validation durch Schleifen selbst implementiert statt z.B. caret bzw. tidymodels dafür zu verwenden. Für die Modellierung wurde &lt;em&gt;Boosting&lt;/em&gt; genutzt. Der verwendete Datensatz stammt aus der Titanic Machine Learning Competition auf Kaggle: &lt;a href=&#34;https://www.kaggle.com/c/titanic/data&#34; class=&#34;uri&#34;&gt;https://www.kaggle.com/c/titanic/data&lt;/a&gt;. Zwar wurde eine Auswahl aus den zur Verfügung stehenden Prädiktoren getroffen. Eine elaborierte analytische Auswahl der zu verwendenden Prädiktoren und deren Transformation wurden allerdings ausgeklammert. Für eine vollumfängliche Analyse inklusive aller Schritte des Predictive Modeling Workflows siehe &lt;a href=&#34;https://trusting-golick-5f2d24.netlify.app/post/random-forest-hyperparametertuning-mit-tidymodels-im-rahmen-der-kaggle-machine-learning-competition-titanic/&#34;&gt;Random Forest: Hyperparametertuning mit tidymodels im Rahmen der Titanic Machine Learning Competition&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Packages&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(knitr)
library(kableExtra)
library(caret)
library(gbm)
library(scales)
library(e1071)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;load-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Load the Data&lt;/h2&gt;
&lt;p&gt;Im ersten Schritt laden wir den Trainingsdatensatz und den Testdatensatz, nachdem wir beide unter &lt;a href=&#34;https://www.kaggle.com/c/titanic/data&#34; class=&#34;uri&#34;&gt;https://www.kaggle.com/c/titanic/data&lt;/a&gt; heruntergeladen haben. Anschließend verknüpfe ich den Trainings- und Testdatensatz zu einem Datensatz, um das Pre-Processing für beide gleichzeitig durchführen zu können. Dazu ist es notwendig, die Observationen der Trainingsdaten als solche kenntlich zu machen und bei den Testdaten die Variable &lt;em&gt;Survived&lt;/em&gt; zu ergänzen, welche in jeder der Testobservationen den Wert NA enthält.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train &amp;lt;- read.csv(file = &amp;quot;train.csv&amp;quot;, 
                          header = TRUE,
                          stringsAsFactors = FALSE)
test &amp;lt;- read.csv(file = &amp;quot;test.csv&amp;quot;, 
                         header = TRUE,
                         stringsAsFactors = FALSE)
PassengerId_submission &amp;lt;- test$PassengerId

train$IsTrain &amp;lt;- rep(TRUE, nrow(train))
test$IsTrain &amp;lt;- rep(FALSE, nrow(test))
test$Survived &amp;lt;- rep(NA, nrow(test))

titanic &amp;lt;- rbind(train, test)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-inspection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Inspection&lt;/h2&gt;
&lt;p&gt;Der Datensatz enthält 13 Variablen (Die Indikatorvariable &lt;em&gt;IsTrain&lt;/em&gt; ausgenommen). Für die Prognose der Variablen &lt;em&gt;Survived&lt;/em&gt; stehen also zunächst 11 Variablen zur Verfügung. Der Testdatensatz umfasst 891 Observationen, der Trainingsdatensatz 418.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(titanic)
head(titanic) 
summary(titanic) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unter den Prädiktorvariablen enthalten &lt;em&gt;Age&lt;/em&gt;, &lt;em&gt;Fare&lt;/em&gt;, &lt;em&gt;Cabin&lt;/em&gt; und &lt;em&gt;Embarked&lt;/em&gt; fehlende Werte. Die Variable &lt;em&gt;Age&lt;/em&gt; enthält 263 fehlende Werte. Bei der Variablen &lt;em&gt;Fare&lt;/em&gt; fehlt lediglich bei einer Observation ein Wert. &lt;em&gt;Cabin&lt;/em&gt; hat 1014 Missings und &lt;em&gt;Embarked&lt;/em&gt; weist 2 Missings auf.&lt;/p&gt;
&lt;p&gt;Bei der ersten Untersuchung ist mir eine Unstimmigkeit in den Daten aufgefallen. Zwei Namen tauchen doppelt auf: Connolly, Miss. Kate und Kelly, Mr. James. Allerdings unterscheiden sich die Ausprägungen der anderen Variablen für die jeweiligen Passagiere mit gleichem Namen. &lt;em&gt;PassengerID&lt;/em&gt;, &lt;em&gt;Age&lt;/em&gt; und &lt;em&gt;Ticketnummer&lt;/em&gt; unterscheiden sich jeweils. Außerdem handelt es sich um nicht unübliche Vor- und Nachnamen, sodass davon auszugehen ist, dass es sich um unterschiedliche Personen gleichen Namens handelt.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;titanic %&amp;gt;% 
  count(Name) %&amp;gt;% 
  filter(n &amp;gt; 1) 

titanic %&amp;gt;% 
  filter(Name %in% duplicate_names)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pre-processing-umgang-mit-missings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pre-Processing: Umgang mit Missings&lt;/h2&gt;
&lt;p&gt;Sowohl bei &lt;em&gt;Age&lt;/em&gt; als auch bei &lt;em&gt;Fare&lt;/em&gt; wurden die fehlenden Werte durch den Median der jeweiligen Variablen ersetzt. Zusätzlich wurde eine neue Variable &lt;em&gt;age_missing&lt;/em&gt; kreiert, die anzeigt, ob die entsprechende Observation einen fehlenden Wert in der Variable &lt;em&gt;Age&lt;/em&gt; aufweist. Für die Variable &lt;em&gt;Cabin&lt;/em&gt; wurde eine neue Faktorstufe für die 1014 fehlenden Werte geschaffen. Observationen mit Missings in Variable &lt;em&gt;Embarked&lt;/em&gt; werden entfernt, da lediglich zwei Observationen Missings in dieser Variable aufweisen. Würde man an dieser Stelle wie bei &lt;em&gt;Cabin&lt;/em&gt; eine neue Faktorstufe (Missing) erzeugen, würde dies später in der Modellierung zu Problemen führen, falls nicht sowohl der Datensatz an dem das Modell trainiert wird als auch der Datensatz, an dem das Modell validiert wird, eine der zwei Observationen mit Ausprägung (Missing) enthalten. Da an späterer Stelle auch Cross-Validation eingesetzt wird, wäre es sehr wahrscheinlich, dass es zu der Situation käme, in der im Testdatensatz Faktorstufen auftauchen, die im Trainingsdatensatz nicht vorkamen.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;titanic %&amp;gt;% 
  mutate(age_missing = is.na(Age)) %&amp;gt;% 
  group_by(age_missing) %&amp;gt;% 
  summarize(surival_rate = mean(Survived, na.rm = T)) 

titanic &amp;lt;- titanic %&amp;gt;% 
  mutate(age_missing = is.na(Age))

titanic$Age[is.na(titanic$Age)] &amp;lt;- median(titanic$Age, na.rm = TRUE)

titanic$Fare[is.na(titanic$Fare)] &amp;lt;- median(titanic$Fare, na.rm = TRUE)

titanic$Cabin[titanic$Cabin == &amp;#39;&amp;#39;] &amp;lt;- NA


titanic &amp;lt;- titanic %&amp;gt;% 
  filter(!(Embarked == &amp;#39;&amp;#39;))  

titanic &amp;lt;- titanic %&amp;gt;% 
  mutate(cabin_missing = is.na(Cabin)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Die kategorialen Variablen wurden zu Faktorvariablen umgewandelt.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;titanic$PassengerId &amp;lt;- factor(titanic$PassengerId)
titanic$Survived &amp;lt;- factor(titanic$Survived)
titanic$Pclass &amp;lt;- factor(titanic$Pclass)
titanic$Sex &amp;lt;- factor(titanic$Sex)
titanic$Embarked &amp;lt;- factor(titanic$Embarked)
titanic$age_missing &amp;lt;- factor(titanic$age_missing)
titanic$cabin_missing &amp;lt;- factor(titanic$cabin_missing)
#titanic$Cabin &amp;lt;- fct_explicit_na(titanic$Cabin)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;modellierung&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modellierung&lt;/h2&gt;
&lt;p&gt;Zur Prognose der Variablen &lt;em&gt;Survived&lt;/em&gt; wurde Boosting verwendet. Zuvor wurde der Gesamtdatensatz nach erfolgter Behandlung der Missings wieder in den Trainingsdatensatz und den Testdatensatz aufgeteilt. Außerdem wurden Variablen, die Passagiere eindeutig oder zumindest sehr genau individuell identifizieren können, nicht als Prädiktoren für die Prognose verwendet, um ein Overfitting der Modelle zu vermeiden. Deshalb wurden die Variablen &lt;em&gt;PassengerId&lt;/em&gt;, &lt;em&gt;Name&lt;/em&gt;, &lt;em&gt;Ticket&lt;/em&gt; und &lt;em&gt;Cabin&lt;/em&gt; entfernt.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unique(titanic$PassengerId) %&amp;gt;% length()
unique(titanic$Name) %&amp;gt;% length()
unique(titanic$Ticket) %&amp;gt;% length()
table(titanic$Cabin)

train &amp;lt;- titanic %&amp;gt;% 
  filter(IsTrain) %&amp;gt;% 
  dplyr::select(-PassengerId, -IsTrain, -Name, -Ticket, -Cabin)

test &amp;lt;- titanic %&amp;gt;% 
  filter(!IsTrain) %&amp;gt;%
  dplyr::select(-PassengerId, -IsTrain, -Name, -Ticket, -Cabin, -Survived)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wir spalten die Trainingsdaten &lt;em&gt;train&lt;/em&gt; in &lt;em&gt;titanic_train&lt;/em&gt; und &lt;em&gt;titanic_test&lt;/em&gt; auf. &lt;em&gt;titanic_train&lt;/em&gt; wird für das Parametertuning per Cross Validation verwendet. Mit &lt;em&gt;titanic_test&lt;/em&gt; bewerten wir das anhand von &lt;em&gt;titanic_train&lt;/em&gt; mit den besten Parameterwerten trainierte Modell. Die Accuracy, die wir beim Test an &lt;em&gt;titanic_test&lt;/em&gt; erhalten, dient als Schätzung für die Accuracy, die bei Submission auf Kaggle zu erwarten ist.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
train_split &amp;lt;- caret::createDataPartition(train$Survived, p = 0.75, list = FALSE)
titanic_train &amp;lt;- train[train_split,]
titanic_test &amp;lt;- train[-train_split,]&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;boosting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Boosting&lt;/h3&gt;
&lt;p&gt;Beim Boosting wird jeder Decision Tree auf einer modifizierten Version der Trainingsdaten trainiert. Bei jedem der trainierten Decision Trees werden die Informationen der vorherigen Trees genutzt, indem die Residuals des vorherigen Trees als abhängige Variable genutzt werden. Somit legt jeder folgende Baum besonderes Gewicht auf die Observationen, die von dem vorherigen Baum schlecht vorhergesagt wurden. Wie schnell dieser Lernvorgang geschieht, wird durch den Hyperparameter &lt;em&gt;shrinkage&lt;/em&gt; bestimmt. Weitere zu bestimmende Hyperparameter sind die Anzahl der Bäume &lt;em&gt;B&lt;/em&gt; und die Anzahl der Splits pro Baum &lt;em&gt;d&lt;/em&gt;. Die Hyperparameter werden durch Cross Validation an &lt;em&gt;titanic_train&lt;/em&gt; bestimmt. Zunächst wird für jeden Hyperparameter eine Sequenz möglicher Werte gebildet und ein Grid names &lt;em&gt;boosting_parameters&lt;/em&gt; erstellt, das alle möglichen Kombinationen der Werte der verschiedenen Hyperparameter enthält. Anschließend bilde ich den Vektor &lt;em&gt;folds&lt;/em&gt; der die Indizes enthält, die genutzt werden, um eine Observation innerhalb der Cross Validation entweder dem Training des jeweiligen Modells oder dem Assessment zuzordnen. Für die Implementierung der Cross Validation verwende ich zwei ineinander verschachtelte for-Schleifen. Die erste SChleife iteriert über alle Kombinationen der Hyperparameter, die zweite Schleife iteriert über alle k Stufen der Cross Validation. Jede der k Folds wird in einer der k Stufen einmal dem Assessment des Modells dienen und in den anderen k-1 Stufen zum Trainieren des Modells verwendet.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ntrees &amp;lt;- 1000
lambda &amp;lt;- c(0.001, 0.005, 0.01, 0.015, 0.02, 0.05, 0.1, 0.15)
n_splits &amp;lt;- seq(1, 10, 1)
boosting_parameters &amp;lt;- expand_grid(ntrees, lambda, n_splits) 
boosting_parameters &amp;lt;- boosting_parameters %&amp;gt;% 
  mutate(id = 1:nrow(boosting_parameters))

set.seed(123)
k &amp;lt;- 5
folds &amp;lt;- sample(1:k, size = nrow(titanic_train), replace = TRUE)
boosting_accuracy &amp;lt;- matrix(NA, nrow = k, ncol = nrow(boosting_parameters),
                            dimnames = list(NULL, boosting_parameters$id)) 

titanic_train_boosting &amp;lt;- titanic_train
titanic_train_boosting$Survived &amp;lt;- as.character(titanic_train_boosting$Survived)


for(i in 1:nrow(boosting_parameters)){
  n.trees &amp;lt;- boosting_parameters$ntrees[i]
  shrinkage &amp;lt;- boosting_parameters$lambda[i]
  interaction.depth &amp;lt;- boosting_parameters$n_splits[i]
  for(j in 1:k){
    df_train &amp;lt;- titanic_train_boosting[j != folds,]
    df_test &amp;lt;- titanic_train_boosting[j == folds,]
    fit &amp;lt;- gbm(Survived ~ .,
               data = df_train,
               n.trees = n.trees,
               interaction.depth = interaction.depth,
               shrinkage = shrinkage)
    pred &amp;lt;- predict(fit,
                    newdata = df_test,
                    type = &amp;quot;response&amp;quot;)
    pred_binary &amp;lt;- ifelse(pred &amp;gt; 0.5, 1, 0) %&amp;gt;% factor(levels = c(0, 1))
    conf_matrix &amp;lt;- caret::confusionMatrix(data = pred_binary,
                           reference = factor(df_test$Survived, levels = c(0,1)),
                           positive = &amp;quot;1&amp;quot;)
    boosting_accuracy[j, i] &amp;lt;- conf_matrix$overall[&amp;quot;Accuracy&amp;quot;]
  }
}

accuracy_mean &amp;lt;- colMeans(boosting_accuracy)
cv_accuracy &amp;lt;- accuracy_mean[which.max(accuracy_mean)]
best_comb_boosting &amp;lt;- boosting_parameters[which.max(accuracy_mean),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nachdem beide Schleifen durchlaufen sind, erhalten wir eine Matrix, die für jede Kombination der Hyperparameter in jeder Stufe der Cross Validation die Accuracy des Modells enthält.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;custom_kable &amp;lt;- function(data, fontsize = 12, fullwidth = FALSE){
  knitr::kable(data) %&amp;gt;% 
    kable_styling(font_size = fontsize, full_width = fullwidth)
}

boosting_accuracy %&amp;gt;% 
  custom_kable()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Die höchste Accuracy wird mit einem Modell erzielt, dass 1000 Bäume mit je 6 Splits und einer Lernrate (shrinkage) von 0.015 verwendet. Alle Observationen, die eine prognostizierte Wahrscheinlichkeit von über 0.5 aufweisen, werden als &lt;em&gt;Überlebend&lt;/em&gt; klassifiziert. Das Modell mit den gemäß Cross Validation besten Parameterwerten für &lt;em&gt;B&lt;/em&gt;, &lt;em&gt;d&lt;/em&gt; und &lt;em&gt;shrinkage&lt;/em&gt; erzielt eine Cross Validation Accuracy von 84.14%.&lt;/p&gt;
&lt;p&gt;Zur abschließenden Bewertung des Modells bewerten wir die Prognosen des Modells mit den soeben gefundenen Hyperparametern an dem Datensatz &lt;em&gt;titanic_test&lt;/em&gt;, der nicht an der Auswahl der Hyperparameter beteiligt war.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ntree_best &amp;lt;- best_comb_boosting[[&amp;quot;ntrees&amp;quot;]] 
n_split_best &amp;lt;- best_comb_boosting[[&amp;quot;n_splits&amp;quot;]]
lambda_best &amp;lt;- best_comb_boosting[[&amp;quot;lambda&amp;quot;]]

boosting_fit &amp;lt;- gbm(Survived ~ .,
                    data = titanic_train_boosting,
                    n.trees = ntree_best,
                    interaction.depth = n_split_best,
                    shrinkage = lambda_best)

pred_boosting_titanic_test &amp;lt;- predict(boosting_fit,
                                      newdata = titanic_test,
                                      type = &amp;quot;response&amp;quot;)
pred_boosting_binary_titanic_test &amp;lt;- ifelse(pred_boosting_titanic_test &amp;gt; 0.5, 1, 0) %&amp;gt;%
  factor(levels = c(0, 1))

conf_matrix &amp;lt;- caret::confusionMatrix(data = pred_boosting_binary_titanic_test,
                                      reference = factor(titanic_test$Survived, levels = c(0,1)),
                                      positive = &amp;quot;1&amp;quot;)

accuracy &amp;lt;- list()
accuracy$boosting &amp;lt;- conf_matrix$overall[&amp;quot;Accuracy&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Die Schätzung der Accuracy beträgt 78.38%. Zum Schluss bestimmen wir mit dem Boosting Modell die Prognosen für den Testdatensatz, speichern sie als csv file ab und reichen sie anschließend auf Kaggle als Submission ein.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_boosting &amp;lt;- train
train_boosting$Survived &amp;lt;- as.character(train_boosting$Survived)

mod_boosting &amp;lt;- gbm(Survived ~ .,
                    data = train_boosting,
                    n.trees = ntree_best,
                    interaction.depth = n_split_best,
                    shrinkage = lambda_best)

pred_boosting_comp &amp;lt;- predict(mod_boosting,
                              newdata = test,
                              type = &amp;quot;response&amp;quot;)
pred_boosting_binary_comp &amp;lt;- ifelse(pred_boosting_comp &amp;gt; 0.5, 1, 0) 

submission_df &amp;lt;- data.frame(PassengerId = PassengerId_submission,
                            Survived = pred_boosting_binary_comp)
write_csv(submission_df,
          path = &amp;quot;submission_boosting_test.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ergebnis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ergebnis&lt;/h2&gt;
&lt;p&gt;Bei der Prognose des Testsets der Competition erzielt unser Boosting Modell mit shrinkage = 0.015, n.trees = 1000 und interaction.depth = 6 eine Accuracy von 77%.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An example preprint / working paper</title>
      <link>/publication/preprint/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/publication/preprint/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-academic&#34;&gt;Create slides in Markdown with Academic&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt; | 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terms</title>
      <link>/terms/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/terms/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example journal article</title>
      <link>/publication/journal-article/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>/publication/journal-article/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>/publication/conference-paper/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>/publication/conference-paper/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
