---
title: 'Random Forest: Hyperparametertuning mit tidymodels im Rahmen der Titanic Machine
  Learning Competition auf Kaggle'
author: Nicolas Mollier
date: '2020-09-02'
slug: random-forest-hyperparametertuning-mit-tidymodels-im-rahmen-der-kaggle-machine-learning-competition-titanic
categories: []
tags:
  - Hyperparametertuning
  - tidymodels
  - Predictive Modeling
subtitle: ''
summary: ''
authors: []
lastmod: '2020-09-02T16:25:09+02:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

Eine Analyse für die Teilnahme an der Titanic Machine Learning Competition auf https://www.kaggle.com/c/titanic. Das Hauptaugenmerk dieser Analyse liegt auf dem Hyperparametertuning per Cross-Validation. Für die Modellierung wird *tidymodels* -der Nachfolger von *caret*- verwendet.


## Packages

```{r Packages, message = FALSE, warning = FALSE}
library(tidyverse)
library(tidymodels)
library(doParallel)
library(vip)
library(kableExtra)
library(modelr)
```


## Get the Data

Im ersten Schritt laden wir den Trainingsdatensatz und den Testdatensatz, nachdem wir beide unter https://www.kaggle.com/c/titanic/data heruntergeladen haben.

```{r Get the Data}
titanic <- read_csv("train.csv")
titanic_holdout <- read_csv("test.csv")
```


## Exploarative Datenanalyse

Wir beginnen mit einer kurzen explorativen Datenanalyse.

Der Datensatz *titanic* enthält Informationen aus `r ncol(titanic)` Variablen für `r nrow(titanic)` Passagiere der Titanic. Abzüglich der Variablen *Survived* und *PassengerId* stehen uns zunächst `r ncol(titanic) - 2` Prädiktoren zur Verfügung.

```{r}
glimpse(titanic)
```

### Beziehung zwischen den Prädiktoren und der zu erklärenden Variablen *Survived*

Als erstes betrachten wir in welcher Beziehung die einzelnen Prädiktoren mit der zu erklärenden Variablen *Survived* stehen. 

```{r EDA 1}

titanic %>% 
  select(Age, Fare, Parch, SibSp, Survived) %>% 
  mutate(Survived = factor(Survived)) %>% 
  pivot_longer(-Survived, names_to = "variable", values_to = "value") %>% 
  ggplot(aes(x = Survived, y = value, fill = Survived)) +
  geom_boxplot() +
  facet_wrap(~variable, scales = "free") +
  theme(axis.title = element_blank())
  ```

Die Grafik zeigt inwieweit sich die Verteilungen der numerischen Varaiblen unterscheiden, je nachedm ob die jeweiligen Passagiere (Observationen) überlebt haben oder nicht. Zur Überprüfung der Unterschiede in Alter und Ticketpreis wurden je eine ANOVA durchgeführt.

```{r, results = 'hide'}

# Age and Survived

anova_age_survived <- anova(aov(Age ~ Survived, data = titanic)) 
p_anova_age_survived <- anova_age_survived$`Pr(>F)`[1]


# Fare and Survived

anova_fare_survived <- anova(aov(Fare ~ Survived, data = titanic))
p_anova_fare_survived <- anova_fare_survived$`Pr(>F)`[1]

```

```{r, include = FALSE}

# Age 

age_per_Survived <- titanic %>% 
  group_by(Survived) %>% 
  summarise(Age = mean(Age, na.rm = TRUE)) 

age_Survived_1 <- age_per_Survived %>% 
  filter(Survived == "1") %>% 
  select(Age) %>% 
  as_vector() %>% 
  round(digits = 1)

age_Survived_0 <- age_per_Survived %>% 
  filter(Survived == "0") %>% 
  select(Age) %>% 
  as_vector() %>% 
  round(digits = 1)

# Fare

fare_per_Survived <- titanic %>% 
  group_by(Survived) %>% 
  summarise(Fare = mean(Fare))

fare_Survived_1 <- fare_per_Survived %>% 
  filter(Survived == "1") %>% 
  select(Fare) %>% 
  as_vector() %>% 
  round(digits = 1)

fare_Survived_0 <- fare_per_Survived %>% 
  filter(Survived == "0") %>% 
  select(Fare) %>% 
  as_vector() %>% 
  round(digits = 1)

# Sex

rate_f <- titanic %>% 
  group_by(Sex) %>% 
  summarise(mean = mean(Survived)) %>% 
  filter(Sex == "female") %>% 
  select(mean) %>% 
  as_vector()

rate_m <- titanic %>% 
  group_by(Sex) %>% 
  summarise(mean = mean(Survived)) %>% 
  filter(Sex == "male") %>% 
  select(mean) %>% 
  as_vector()
```

Die Überlebenden waren zwar statistisch signifikant jünger (ANOVA: p-Wert = `r scales::percent(p_anova_age_survived, accuracy = 0.01)`). Allerdings beträgt der Unterschied nur `r age_Survived_0 - age_Survived_1` Jahre (`r age_Survived_1` vs. `r age_Survived_0`) Die Überlebenden besaßen aber deutlich höherpreisige Tickets als die Verstorbenen (`r fare_Survived_1` vs. `r fare_Survived_0`, ANOVA: p-Wert < 0.01).


Die nachfolgende Grafik zeigt die Beziehung der Variablen *Embarked*, *Pclass* und *Sex* mit der zu erklärenden Variablen *Survived*. Auf der y-Achse sind die Überlebensraten abgetragen.

```{r, fig.height=3}

titanic %>% 
  select(Embarked, Sex, Pclass, Survived) %>% 
  filter(Embarked != "NA") %>% 
  mutate_at(vars(Embarked, Sex, Pclass), as.factor) %>% 
  pivot_longer(-Survived, names_to = "variable", values_to = "value") %>% 
  group_by(variable, value) %>% 
  summarise(mean = mean(Survived, na.rm = T)) %>% 
  mutate(variable = factor(variable)) %>% 
  ggplot(aes(value, mean)) +
  geom_bar(stat = "identity", na.rm = T) +
  facet_grid(~variable, scale = "free_x", space = "free") +
  labs(x = "", y = "Überlebensrate")

```

Passagiere der ersten Klasse überlebten deutlich häufiger als die Passagiere der zweiten bzw. dritten Klasse. Am deutlichsten ist der Unterschied der Überlebensrate zwischen Männern und Frauen. Während Frauen zu `r scales::percent(rate_f, accuracy = 0.01)` überlebten, waren es unter den männlichen Passagieren nur `r scales::percent(rate_m, accuracy = 0.01)`.

### Kollinearität der Prädiktoren

Wir werfen einen kurzen Blick auf die Kollinearität der Prädiktoren.

```{r Collinearity of numeric variables, fig.height=4, fig.width=4}

titanic %>% 
  select(Age, Fare, Parch, SibSp, Pclass) %>% 
  na.omit() %>% 
  cor() %>% 
  corrplot::corrplot.mixed()

```

Die numerischen Prädiktoren weisen keine erhöhten Korrelationen untereinander auf. Dennoch werden wir zur Sicherheit vor der Modellierung im Schritt *recipe* einen Algorithmus auf die Daten anwenden, der Prädiktoren mit zu großer absoluter Korrelation mit anderen Prädiktoren entfernt. 

### Schiefe der Verteilungen

Als nächstes betrachten wir die Verteilungen der Prädiktorvariablen. Zu schiefe Verteilungen können bei der Prognose zu Problemen führen.

```{r Distribution of Predictors: Skewness}

titanic %>% 
  select(Age, Fare, SibSp, Parch) %>%
  pivot_longer(1:4, names_to = "variable", values_to = "value") %>% 
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~variable, scales = "free") +
  labs(x = "", y = "")

```

Insbesondere die Variable *Fare* weist eine enorm rechtsschiefe Verteilung auf. Deshalb werden wir vor der Modellierung die Yeo-Johnson Transformation anwenden, um die Verteilung von *Fare* symmetrischer zu machen.


## Entfernung von Variablen

### Overfitting

Um ein Overfitting zu vermeiden, möchten wir Variablen, die den jeweiligen Passagier eindeutig oder nahezu eindeutig identifizieren, nicht zur Modellierung verwenden. Um diese Variablen herauszufinden, betrachten wir die Anzahl der "unique values" der Variablen des Typs *character*.

```{r}
titanic %>% 
  select_if(is.character) %>% 
  map(unique) %>% 
  map_df(length) %>% 
  pivot_longer(Name:Embarked, names_to = "variable", values_to = "# unique values") %>% 
  knitr::kable()
```

Die Variablen mit Typ *character* haben teilweise sehr viele verschiedene Werte. Mit den Variablen *Name* und *Ticket* sind die jeweiligen Passagiere nahezu eindeutig identifizierbar. Um ein Overfitting zu vermeiden, werden *Name* und *Ticket* deshalb nicht zur Modellierung benutzt.

## Missings

Für die Schätzung (Imputation) fehlender Werte verwenden wir *Bagged Tree Imputation*. Enthält eine Variable allerdings zu viele Missings, so macht selbst eine Schätzung der fehlenden Werte keinen Sinn mehr und die Variable sollte entfernt werden.

```{r}
na_prop_cabin <- titanic %>% 
  count(Cabin) %>% 
  mutate(prop = n / sum(n)) %>% 
  arrange(desc(prop)) %>% 
  slice_head(n = 1) %>% 
  select(prop) %>% 
  as_vector()
```

Die Variable *Cabin* enthält zu `r scales::percent(na_prop_cabin, accuracy = 0.01)` fehlende Werte (NAs). Aus diesem Grund wird auch diese Variable nicht zur Modellierung verwendet.

Insegesamt werden also *Cabin*, *Name* und *Ticket* aus dem Datensatz entfernt.

```{r}
titanic <- titanic %>%
  select(-Cabin, -Name, -Ticket)
```

Neben der bereits entfernten Variablen *Cabin* weisen noch zwei weitere Variablen fehlende Werte auf.

```{r, Find Missings}
summary(titanic)
```

```{r, include = FALSE}
na_age <- is.na(titanic$Age) %>% sum()
na_embarked <- is.na(titanic$Embarked) %>% sum()
```

Die Variable *Age* hat `r na_age` Observationen mit fehlendem Wert. Bei Variablen des Typs *character* sind fehlende Werte durch bloßen Aufruf der Funktion *summary()* nicht erkennbar. Durch zweifachen Aufruf der Funktion *map()* können alle verbleibenden Variablen mit fehlenden Werten identifiziert werden. 

```{r, Find Missings 2}
map(titanic, is.na) %>% 
  map_df(sum) %>% 
  pivot_longer(PassengerId:Embarked, names_to = "variable", values_to = "missings") %>% 
  filter(missings > 0) %>% 
  knitr::kable()
```

Die Variable *Embarked* besitzt `r na_embarked` fehlende Werte. Im Schritt *recipe* wird die Schätzung der fehlenden Werte der Variablen *Age* und *Embarked* mithilfe von *Bagged Trees* anhand der Ausprägungen der anderen Variablen des jeweiligen Passagiers spezifiziert.

## Data Split

Bevor wir mit der Modellierung beginnen, wird der Datensatz *titanic* in den Trainingsdatensatz *titanic_train* und den Testdatensatz *titanic_test* aufgesplittet. Dabei wird *Stratified Sampling* verwendet, um zu erreichen, dass die Verteilung von Überlebenden und Verstorbenen in beiden Datensätzen etwa gleich groß ist. 

```{r Split}
set.seed(1234)
titanic_split <- initial_split(titanic, strata = Survived)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
```

Insgesamt haben wir also drei Datensätze: Den Trainingsdatensatz *titanic_train* an dem wir mit 10-fold Cross Validation die Werte der jeweiligen Hyperparameter der Modelle bestimmen. Der Testdatensatz *titanic_test* andem wir die Performance des durch Cross Validation gefundenen besten Modells auf einem, dem Modell unbekannten, Datensatz testen. Und den Datensatz *titanic_holdout*, bei dem wir nicht wissen, ob die Passagiere überlebt haben oder nicht und für den wir diese Werte im Rahmen der Kaggle Competition prognostizieren müssen.


## Modellierung

Die Modellierung mit *tidymodels* kann in folgende Schritte unterteilt werden. Zunächst wird ein sogenanntes *Rezept* erstellt, dass alle Pre-Processing Schritte wie Imputation und Transformationen von Variablen umfasst. Danach wird die sogenannte Modellspezifikation vorgenommen. Danach werden *Rezept* und *Modellspezifikation* im sogenannten *Workflow* zusammengefasst. Dieser Workflow wird dann genutzt, um das Hyperparametertuning, die Validierung des besten Modells und die abschließende Prognose für die Competition zu erstellen.

### Recipe

Zunächst wird mit der Funktion *recipe()* festgelegt, welche Pre-Processing Schritte durchzuführen sind. Hier wird umgesetzt, was wir im Rahmen der explorativen Datenanalysen herausgefunden haben. *PClass* und *Survived* werden zu Faktorvariablen umgewandelt. Fehlende Werte der Variablen *Age* und *Embarked* werden per *Bagging* geschätzt und auf die Variable *Fare* wird die Yeo-Johnson Transformation angewendet.

```{r Recipe}
titanic_rec <- recipe(Survived ~ ., data = titanic_train) %>%
  update_role(PassengerId, new_role = "ID") %>% 
  step_mutate(Pclass = factor(Pclass, labels = c("first", "second", "third")) %>% 
                fct_rev()) %>% 
  step_mutate(Survived = factor(Survived)) %>% 
  step_bagimpute(all_predictors()) %>%
  step_YeoJohnson(Fare)  %>% 
  #step_dummy(Pclass, Embarked, Sex) %>% 
  prep()
```

Um zu überprüfen, welche Auswirkung die Yeo-Johnson Transformation auf die Verteilung der Variablen *Fare* hat, betrachten wir folgende Grafik. Die Verteilung ist immer noch rechtsschief, aber die Schiefe wurde deutlich reduziert.

```{r, fig.height=3}
bake(titanic_rec, new_data = titanic) %>% 
  select(Fare) %>% 
  mutate(Fare_original = titanic$Fare) %>% 
  rename(Fare_transformed = Fare) %>% 
  pivot_longer(1:2, names_to = "variable", values_to = "values") %>% 
  mutate(variable = factor(variable)) %>% 
  ggplot(aes(x = values, fill = variable)) +
  geom_histogram() +
  facet_grid(~variable, scales = "free") +
  theme(legend.position = "none") %>% 
  labs(x = "", y = "")
  
```

### Modellspezifikation und Workflow

Im nächsten Schritt wird das Modell spezifiziert. Wir wählen ein Random Forest Modell zur Klassifizierung, welches als Engine das Paket *ranger* verwendet. Die Hyperparameter *mtry*, *trees* und *min_n* erhalten Platzhalter, da sie erst noch im nächsten Schritt per Cross Validation bestimmt werden. 

```{r}

rf_spec <- rand_forest(
  mode = "classification",
  mtry = tune(),
  trees = tune(),
  min_n = tune()) %>% 
  set_engine("ranger", importance = "impurity")

```

Das angefertigte Rezept und die Modellspezifikation werden im sogenannten Workflow zusammengefasst.

```{r}

rf_wflow <- workflow() %>% 
  add_recipe(titanic_rec) %>% 
  add_model(rf_spec)

```

### Hyperparametertuning

Zur Durchführung des Hyperparametertuning werden 10 Folds aus dem Trainingsdatensatz gebildet, die jeweils in Analyseset und Assessmentset unterteilt sind. Je Fold wird das Analyseset zum Trainng des Modells genutzt, während anhand des Assessmentsets die Performance bestimmt wird. Auch hier nutzen wir *Stratified Sampling*.

```{r}
set.seed(1234)
titanic_train_folds <- vfold_cv(titanic_train, strata = Survived)
titanic_train_folds 
```

Wir bilden ein Grid, dass alle Kombinationen aus den Werten der Hyperparameter enthält. Wir versuchen 7 verschiedene Werte für die Anzahl der pro Split des jeweiligen Decision Trees verwendeten Prädiktoren (*mtry*), 6 verschiedene Werte für die Anzahl der pro Random Forest verwendeten Decision Trees (*trees*) und 8 verschiedene Werte für die minimale Anzahl an Observationen, die in einem Knoten eines Decision Trees notwendig ist, um einen Split dieses Knotens in Erwägung zu ziehen. Die Spanne der betrachteten Werte für *mtry* wird auf 1 bis 7 festgelegt.

```{r}
rf_param <- parameters(rf_spec) %>% 
  update(mtry = mtry(range = c(1, 7)),
         trees = trees(range = c(1, 2500))) %>% 
  finalize(x = titanic_train %>% select(-Survived))

grid_rf <- grid_regular(rf_param, levels = c(mtry = 7, trees = 6, min_n = 8))

grid_rf %>% 
  knitr::kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover")) %>% 
  scroll_box(width = "35%", height = "300px")
```

Insgesamt erhalten wir `r nrow(grid_rf)` Kombinationsmöglichkeiten für die Hyperparameter. Für jede dieser Kombinationen wird pro Fold ein Random Forest mit jeweils bis zu 2500 Decision Trees trainiert. Das ergibt insgesamt `r nrow(grid_rf) * 10` Random Forests. Um die Zeit zu reduzieren, die für das Trainieren all dieser Random Forests bzw. Decision Trees notwendig ist, verwenden wir *Parallel Processing*.

```{r}
all_cores <- parallel::detectCores(logical = FALSE) 
doParallel::registerDoParallel(cores = all_cores)
```

In diesem Schritt findet das Training der `r nrow(grid_rf) * 10` Random Forests im Rahmen des Hyperparametertunings statt. Dazu wird der bereits definierte Workflow zusammen mit dem bereits spezifizierten Grid der zu testenden Parameterwerte und den gebildeten Folds in der Funktion *tune_grid* verwendet.

```{r}

tune_rf <- tune_grid(
  rf_wflow,
  resamples = titanic_train_folds,
  grid = grid_rf,
  control = control_grid(save_pred = T)
)

```

### Visualisierung

Nachdem die Modelle trainiert wurden, visualisieren wir die Ergebnisse der Cross Validation. 

#### Accuracy und AUC

Als mögliche Werte für die Anzahl der verwendeten Trees pro Random Forest wurden die Werte 1, 500, 1000, 1500, 2000 und 2500 verwendet. 

```{r}
tune_rf %>% 
  collect_metrics() %>% 
  filter(.metric == "accuracy") %>% 
  mutate(mtry = factor(mtry)) %>% 
  ggplot(aes(x = min_n, y = mean, color = mtry)) +
  geom_line() +
  geom_point() +
  facet_wrap(~trees, labeller = "label_both") +
  labs(x = "Minimal Node Size (min_n)",
       y = "Accuracy")
```

Die Grafik zeigt, dass -wie zu erwarten- die am schlechtesten performenden Modelle solche sind, die nur aus einem Tree bestehen. Ob 500, 1000, 15000, 2000 oder 2500 Trees pro Random Forest verwendet werden, scheint keinen Unterschied zu machen. Die Modelle mit einer *Minimal Node Size* (*min_n*) von etwa 20 bis 25 scheinen die besten Ergebnisse für die Metrik *Accuracy* zu erzielen. Für die Anzahl der pro Knoten betrachteten Prädiktoren scheint ein Wert von etwa 3 gut zu funktionieren, wenn man die Kurven für die Accuracy betrachtet. Dieser Eindruck wird durch die nachfolgende Grafik bestätigt.

```{r, fig.height = 3}
tune_rf %>% 
  collect_metrics() %>%
  filter(.metric == "accuracy" & trees != 1) %>% 
  select(mean, mtry:min_n) %>% 
  pivot_longer(mtry:min_n, 
               names_to = "parameter",
               values_to = "value") %>% 
  ggplot(aes(value, mean, color = parameter)) +
  geom_point() +
  geom_smooth() +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = "",
       y = "Accuracy")
```

#### ROC 

Wir visualisieren die ROC Kurven, wobei wir jeweils die Kurven durch eine der drei Hyperparameter gruppieren und einfärben. Für die Metrik *Area under the Curve (AUC)* spielt die Wahl der Hyperparameter insgesamt keine große Rolle, solange man mindestens 500 Trees pro Random Forest verwendet.

```{r}
tune_rf %>% 
  collect_predictions() %>% 
  mutate(mtry = factor(mtry)) %>% 
  group_by(mtry) %>% 
  roc_curve(truth = Survived, .pred_1, event_level = "second") %>% 
  ggplot(aes(x = 1- specificity, y = sensitivity, color = mtry)) +
  geom_path() +
  geom_abline(lty = 3, alpha = 0.6) +
  coord_equal()


tune_rf %>% 
  collect_predictions() %>% 
  mutate(min_n = factor(min_n)) %>% 
  group_by(min_n) %>% 
  roc_curve(truth = Survived, .pred_1, event_level = "second") %>% 
  ggplot(aes(x = 1- specificity, y = sensitivity, color = min_n)) +
  geom_path() +
  coord_equal() +
  geom_abline(lty = 3, alpha = 0.6) 

  
tune_rf %>% 
  collect_predictions() %>% 
  mutate(trees = factor(trees)) %>% 
  group_by(trees) %>% 
  roc_curve(truth = Survived, .pred_1, event_level = "second") %>% 
  ggplot(aes(x = 1- specificity, y = sensitivity, color = trees)) +
  geom_path() +
  coord_equal() +
  geom_abline(lty = 3, alpha = 0.6) 


```

Was den Verlauf der *ROC Kurve* betrifft, so hat *mtry* keinen wesentlichen Einfluss, solange mtry größer als 1 ist. Die Roc Kurven unterschieden sich für verschiedene Werte von *min_n* ebenfalls nur geringfügig. Die Anzahl der Bäume pro Random Forest spielt für den Verlauf der ROC Kurve keine Rolle, solange die Anzahl mindestens 500 beträgt.

### Auswahl der Werte der Hyperparameter

Nachdem wir visuell untersucht haben, welche Hyperparameterwerte sinnvoll erscheinen, wählen wir nun die endgültigen Werte der Hyperparameter. Dazu verwenden wir als Metriken sowohl die durchschnittliche Accuracy über alle 10 Folds, als auch den Standardfehler der 10 Accuracy Werte pro Parameterkombination. Zwar möchten wir ein Modell mit möglichst hoher durchschnittlicher Accuracy, allerdings ist es auch wichtig, eine Parameterkombination zu wählen, die bei Anwendung auf verschiedenen Datensätzen keine allzu schwankende Performance liefert. Dies ist insbesondere unter dem Aspekt wichtig, dass wie bei der Kaggle Competition Prognosen für nur einen Datensatz liefern müssen bzw. können und nicht wie bei der Cross Validation sozusagen 10 Versuche pro Parameterkombination haben, wobei nur der Durchschnittswert zählt. Dazu speichern wir jeweils die Parameterwerte der gemäß *Accuracy* bzw. *Standardfehler der Accuracywerte* 20 besten Modelle. 

```{r Select Best Hyperparameter Values}

cv_rf <-tune_rf %>% 
  collect_metrics()

best_accuracy <- tune_rf %>% 
  show_best(metric = "accuracy", n = 20) %>% 
  arrange(std_err)

best_accuracy %>% 
  knitr::kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover"),
                font_size = 13) %>% 
  scroll_box(height = "300px")

```

Die gemäß *Accuracy* besten Modelle weisen eine über alle 10 Folds gemittelte Accuracy zwischen 83% und 84% auf. Die Darstellung erfolgt nach Streuung (std_err) sortiert.

```{r}
best_std <- cv_rf %>% 
  filter(.metric == "accuracy") %>% 
  arrange(std_err) %>% 
  head(n = 20) 
best_std %>% 
  knitr::kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover"),
                font_size = 13) %>% 
  scroll_box(height = "300px")

  
best_params <- best_accuracy %>% 
  slice_head(n = 1) %>% 
  select(mtry, trees, min_n)

```

Bei den gemäß *Standardfehler der Accuracywerte* besten Modellen beträgt die durchschnittliche Accuracy zwischen 80% und 82%. Als endgültige Parameter wählen wir die Werte des Modells, das unter den gemäß Accuracy 20 besten Modellen die geringste Streuung aufweist. Dieses Modell erzielte bei der Cross Validation eine deutlich überdurchschnittliche Accuracy und gleichzeitig eine unterdurchschnittliche Streuung. Der jeweilige Wert des gewählten Modells ist in den folgenden Boxplots durch einen roten Punkt gekennzeichnet.

```{r, fig.height = 3}
best_params_metrics <- best_accuracy %>% 
  slice_head(n = 1) %>% 
  select(Accuracy = mean, Streuung = std_err)

ref_point <- best_params_metrics %>% 
  pivot_longer(Accuracy:Streuung, names_to = "metric", values_to = "values")

cv_rf %>% 
  filter(.metric == "accuracy") %>% 
  select(Accuracy = mean, Streuung = std_err) %>% 
  pivot_longer(Accuracy:Streuung, names_to = "metric", values_to = "value") %>% 
  ggplot(aes(value)) +
  geom_boxplot() +
  geom_point(data = ref_point, aes(x = values, y = 0), color = "red", size = 3) +
  facet_wrap(~metric, scales = "free") +
  labs(x = "", y = "")
```


Somit verwenden wir sowohl zur Validierung am Testdatensatz als auch zur Prognose am Datensatz *titanic_holdout* die Parameterwerte:

* mtry = `r best_params %>% select(mtry) %>% as_vector()`
* trees = `r best_params %>% select(trees) %>% as_vector()`
* min_n = `r best_params %>% select(min_n) %>% as_vector()`

Abschließend testen wir das Modell an dem Testdatensatz *titanic_test*, der nicht zur *Model Selection* verwendet wurde. Dieser Test dient dazu, eine realistische Einschätzung bezüglich der Performance unseres Modells an neuen Daten zu erhalten.

```{r Modeltest}
set.seed(1234)

rf_fit_split <- rf_wflow %>%
  finalize_workflow(parameters = best_params) %>%
  last_fit(split = titanic_split)

test_metrics <- rf_fit_split %>%
  collect_metrics() 

test_metrics %>% 
  knitr::kable()
```
        
```{r, include = FALSE}
test_accuracy <- 
  test_metrics %>% 
  filter(.metric == "accuracy") %>% 
  select(.estimate) %>% 
  as_vector() %>% 
  scales::percent(accuracy = 0.1)
```

Bei der Prognose an dem Testdatensatz *titanic_test* erzielt das Modell eine Accuracy von `r test_accuracy`.

Die wichtigsten Prädiktoren unseres Modells können mit der Funktion *vip()* visualisiert werden.

```{r, fig.height = 3}
rf_fit_split %>%
  pluck(".workflow", 1) %>%
  pull_workflow_fit() %>%
  vip(num_features = 7)
```

Die wichtigste Variable ist mit Abstand das Geschlecht, gefolgt von der Höhe des Ticketpreises und dem Alter.

## Kaggle Submission

Abschließend erstellen wir die *Prediction* für die Submission auf Kaggle.

```{r}
last_rf_fit <- rf_wflow %>%
  finalize_workflow(parameters = best_params) %>%
  fit(data = titanic)

titanic_holdout <- titanic_holdout %>% 
  mutate(Survived = NA)

final_pred <- last_rf_fit %>% 
  extract_model() %>% 
  predict(bake(titanic_rec, titanic_holdout), type = "response") 
  
final_pred_binary <- 
  ifelse(final_pred$predictions[,2] > 0.5, 1, 0) %>% 
  factor(levels = c(0, 1))

submission_df <- data.frame(PassengerId = titanic_holdout$PassengerId,
                            Survived = final_pred_binary)

write_csv(submission_df, "submission_kaggle.csv")
```

```{r include = FALSE}
leaderboard <- read_csv("leaderboard.csv")
leaderboard %>% 
  filter(Score > 0.5 & Score < 0.9) %>% 
  ggplot(aes(x = Score)) +
  geom_histogram()

quantile(leaderboard$Score, probs = seq(0.6, 0.95, 0.05)) 

percentile_Score <- leaderboard %>% 
  # filter(Score < 0.9) %>% 
  arrange(desc(Score)) %>% 
  mutate(x = Score > 0.79186) %>% 
  summarize(mean = mean(x)) %>% 
  as_vector() %>% 
  scales::percent(accuracy = 1)
```


Das Modell erzielt bei der Competition eine Accuracy von 79.2%. Damit liegt das Modell unter den Top `r percentile_Score` der Submissions.


